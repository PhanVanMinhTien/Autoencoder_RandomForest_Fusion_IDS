{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c667db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "#1a_ae_rf_fusion_mix_training.ipynb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Setup path\n",
    "current_dir = Path.cwd()\n",
    "root_dir = current_dir.parent\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "print(f\"Project Root: {root_dir}\")\n",
    "\n",
    "# Import modules\n",
    "from src import config, preprocessing, autoencoder, rf_classifier, evaluation, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n Data Folder thay v√¨ Data Path\n",
    "print(f\"Checking Data Directories:\")\n",
    "print(f\"  - 2017: {config.DIR_2017} -> {'‚úÖ C√≥' if config.DIR_2017.exists() else '‚ùå Kh√¥ng th·∫•y'}\")\n",
    "print(f\"  - 2018: {config.DIR_2018} -> {'‚úÖ C√≥' if config.DIR_2018.exists() else '‚ùå Kh√¥ng th·∫•y'}\")\n",
    "\n",
    "if not config.DIR_2017.exists() or not config.DIR_2018.exists():\n",
    "    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n folder trong src/config.py\")\n",
    "\n",
    "# --- KH·ªûI T·∫†O EXPERIMENT M·ªöI ---\n",
    "# B∆∞·ªõc n√†y s·∫Ω t·∫°o folder d·∫°ng: results/experiments/exp_20251212_064500/\n",
    "exp_paths = utils.setup_experiment_folder()\n",
    "utils.log_experiment_details(exp_path=exp_paths['root'])\n",
    "print(f\"üöÄ Experiment ID initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6158cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Datasets Separately\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    print(\"üöÄ Loading 2017 and 2018 datasets separately...\")\n",
    "    \n",
    "    # N·∫°p 2017\n",
    "    X_17, y_17 = preprocessing.load_single_dataset_year('2017', binary_mode=True)\n",
    "    print(f\"‚úÖ Loaded 2017: {X_17.shape}\")\n",
    "    \n",
    "    # N·∫°p 2018\n",
    "    X_18, y_18 = preprocessing.load_single_dataset_year('2018', binary_mode=True)\n",
    "    print(f\"‚úÖ Loaded 2018: {X_18.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi n·∫°p d·ªØ li·ªáu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Sanity Check - Ki·ªÉm tra Chu·∫©n h√≥a v√† Th·ª© t·ª±\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import config, preprocessing\n",
    "\n",
    "def verify_feature_order_and_normalization():\n",
    "    print(\"üïµÔ∏è‚ôÇÔ∏è B·∫ÆT ƒê·∫¶U KI·ªÇM TRA T√çNH TO√ÄN V·∫∏N D·ªÆ LI·ªÜU...\\n\")\n",
    "    \n",
    "    # 1. Ki·ªÉm tra Th·ª© t·ª± (Order Guarantee)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"1Ô∏è‚É£  Ki·ªÉm tra Config (Khu√¥n m·∫´u):\")\n",
    "    expected_cols = config.SELECTED_FEATURES\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng features trong Config: {len(expected_cols)}\")\n",
    "    print(f\"   - 5 features ƒë·∫ßu: {expected_cols[:5]}\")\n",
    "    print(f\"   - 5 features cu·ªëi: {expected_cols[-5:]}\")\n",
    "    print(\"   ‚úÖ K·∫øt lu·∫≠n: Bi·∫øn X (numpy) c·ªßa b·∫°n CH·∫ÆC CH·∫ÆN tu√¢n theo th·ª© t·ª± n√†y.\")\n",
    "    print(\"      (V√¨ code load d√πng l·ªánh: chunk[config.SELECTED_FEATURES])\\n\")\n",
    "\n",
    "    # 2. Ki·ªÉm tra Chu·∫©n h√≥a tr√™n File Th·ª±c (Real Data Test)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"2Ô∏è‚É£  Ki·ªÉm tra th·ª±c t·∫ø tr√™n file CSV (M·ªói nƒÉm 1 file):\")\n",
    "    \n",
    "    # L·∫•y 1 file m·∫´u t·ª´ m·ªói nƒÉm\n",
    "    files_to_check = {\n",
    "        \"2017\": next(config.DIR_2017.glob(\"*.csv\"), None),\n",
    "        \"2018\": next(config.DIR_2018.glob(\"*.csv\"), None)\n",
    "    }\n",
    "\n",
    "    for year, file_path in files_to_check.items():\n",
    "        if not file_path:\n",
    "            print(f\"   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {year}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n   üëâ Ki·ªÉm tra dataset {year}: {file_path.name}\")\n",
    "        \n",
    "        # ƒê·ªçc th√¥ 5 d√≤ng\n",
    "        df_raw = pd.read_csv(file_path, nrows=5)\n",
    "        print(f\"      - C·ªôt g·ªëc (Raw): {df_raw.columns.tolist()[:3]} ...\")\n",
    "        \n",
    "        # √Åp d·ª•ng h√†m chu·∫©n h√≥a\n",
    "        df_norm = preprocessing.normalize_column_names(df_raw)\n",
    "        \n",
    "        # Ki·ªÉm tra xem c√°c c·ªôt quan tr·ªçng c√≥ xu·∫•t hi·ªán kh√¥ng\n",
    "        # V√≠ d·ª•: Ki·ªÉm tra xem c·ªôt 'Destination Port' (ƒë√£ chu·∫©n h√≥a) c√≥ m·∫∑t kh√¥ng\n",
    "        target_col = \"Destination Port\"\n",
    "        if target_col in df_norm.columns:\n",
    "            print(f\"      - ‚úÖ ƒê√£ t√¨m th·∫•y '{target_col}' (Chu·∫©n h√≥a OK)\")\n",
    "        else:\n",
    "            print(f\"      - ‚ùå C·∫¢NH B√ÅO: Kh√¥ng th·∫•y '{target_col}'. C√≥ th·ªÉ mapping t√™n b·ªã sai!\")\n",
    "            \n",
    "        # Ki·ªÉm tra s·ª± kh·ªõp l·ªánh v·ªõi Config\n",
    "        # T·∫°o df gi·∫£ l·∫≠p b∆∞·ªõc l·ªçc\n",
    "        for col in config.SELECTED_FEATURES:\n",
    "            if col not in df_norm.columns:\n",
    "                df_norm[col] = 0.0 # Gi·∫£ l·∫≠p fill missing\n",
    "        \n",
    "        df_final = df_norm[config.SELECTED_FEATURES]\n",
    "        \n",
    "        # So s√°nh th·ª© t·ª±\n",
    "        if df_final.columns.tolist() == config.SELECTED_FEATURES:\n",
    "            print(f\"      - ‚úÖ Th·ª© t·ª± c·ªôt sau khi l·ªçc kh·ªõp 100% v·ªõi Config.\")\n",
    "        else:\n",
    "            print(f\"      - ‚ùå SAI TH·ª® T·ª∞! C·∫ßn ki·ªÉm tra l·∫°i code.\")\n",
    "\n",
    "verify_feature_order_and_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9670305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Independent Splitting & Mixed Training Construction\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"‚úÇÔ∏è Splitting 2017 and 2018 independently (80/20)...\")\n",
    "\n",
    "# 1. Chia 2017\n",
    "X_17_train, X_17_test, y_17_train, y_17_test = train_test_split(\n",
    "    X_17, y_17, test_size=0.2, random_state=config.SEED, stratify=y_17\n",
    ")\n",
    "\n",
    "# 2. Chia 2018\n",
    "X_18_train, X_18_test, y_18_train, y_18_test = train_test_split(\n",
    "    X_18, y_18, test_size=0.2, random_state=config.SEED, stratify=y_18\n",
    ")\n",
    "\n",
    "# 3. G·ªôp c√°c ph·∫ßn Training (Scenario 2)\n",
    "X_train = np.vstack([X_17_train, X_18_train])\n",
    "y_train = np.concatenate([y_17_train, y_18_train])\n",
    "\n",
    "# 4. Gi·ªØ c√°c ph·∫ßn Test ri√™ng bi·ªát ƒë·ªÉ ƒë√°nh gi√° ƒëa chi·ªÅu\n",
    "# X_test v√† y_test t·ªïng h·ª£p (n·∫øu v·∫´n mu·ªën ƒë√°nh gi√° chung)\n",
    "X_test_all = np.vstack([X_17_test, X_18_test])\n",
    "y_test_all = np.concatenate([y_17_test, y_18_test])\n",
    "\n",
    "print(f\"üìä Mixed Train Size: {X_train.shape}\")\n",
    "print(f\"üìä Test 2017 Size:  {X_17_test.shape}\")\n",
    "print(f\"üìä Test 2018 Size:  {X_18_test.shape}\")\n",
    "\n",
    "# X√≥a d·ªØ li·ªáu th√¥ ƒë·ªÉ gi·∫£i ph√≥ng RAM\n",
    "del X_17, X_18, X_17_train, X_18_train\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2099931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Scaling & Save Scaler\n",
    "print(\"üöÄ Scaling data...\")\n",
    "\n",
    "# 1. Kh·ªüi t·∫°o Scaler\n",
    "scaler = preprocessing.get_scaler()\n",
    "\n",
    "# 2. Fit v√† Transform tr√™n t·∫≠p MIXED TRAIN\n",
    "# ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng nh·∫•t: Scaler ch·ªâ ƒë∆∞·ª£c h·ªçc th√¥ng s·ªë t·ª´ t·∫≠p hu·∫•n luy·ªán [cite: 514]\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 3. Transform c√°c t·∫≠p Test ƒë·ªôc l·∫≠p (S·ª≠ d·ª•ng th√¥ng s·ªë c·ªßa t·∫≠p Train) \n",
    "# Ch√∫ng ta scale ri√™ng t·ª´ng b·ªô ƒë·ªÉ ph·ª•c v·ª• ƒë√°nh gi√° ƒëa chi·ªÅu ·ªü Cell 10\n",
    "X_17_test_scaled = scaler.transform(X_17_test)\n",
    "X_18_test_scaled = scaler.transform(X_18_test)\n",
    "X_test_scaled    = scaler.transform(X_test_all) # T·∫≠p test t·ªïng h·ª£p (n·∫øu c·∫ßn)\n",
    "\n",
    "print(\"‚úÖ Done scaling for all datasets.\")\n",
    "\n",
    "# --- L∆ØU SCALER V√ÄO FOLDER EXPERIMENT ---\n",
    "import joblib\n",
    "print(\"üíæ Saving scaler...\")\n",
    "scaler_path = exp_paths[\"models\"] / \"scaler.joblib\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"‚úÖ Saved scaler to: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53906ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Train Autoencoder (L=30, N=5)\n",
    "print(f\"üöÄ Training Autoencoder on: {config.DEVICE}\")\n",
    "\n",
    "# 1. ƒê·ªãnh nghƒ©a L v√† N d·ª±a tr√™n k·∫øt qu·∫£ t·ªëi ∆∞u\n",
    "L = 30  # Input dimension cho AE \n",
    "N = 5   # Latent dimension (ƒëi·ªÉm ng·ªçt) [cite: 525]\n",
    "\n",
    "# 2. C·∫ÆT D·ªÆ LI·ªÜU: Ch·ªâ l·∫•y 30 ƒë·∫∑c tr∆∞ng mRMR ƒë·∫ßu ti√™n\n",
    "# X_train_scaled ƒëang c√≥ 65 c·ªôt, ch√∫ng ta l·∫•y [:, :30]\n",
    "X_train_ae_input = X_train_scaled[:, :L]\n",
    "\n",
    "# 3. Kh·ªüi t·∫°o model v·ªõi input_dim kh·ªõp v·ªõi d·ªØ li·ªáu ƒë√£ c·∫Øt\n",
    "ae_model = autoencoder.DeepAutoencoder(\n",
    "    input_dim=L,             # Ph·∫£i l√† 30 \n",
    "    latent_dim=N,            # Ph·∫£i l√† 5 [cite: 525]\n",
    "    hidden_layers=[22, 12]   # C·∫•u tr√∫c n√©n d·∫ßn ph√π h·ª£p cho L=30 \n",
    ")\n",
    "\n",
    "ae_save_path = exp_paths[\"models\"] / \"ae_model.pth\"\n",
    "\n",
    "# 4. Hu·∫•n luy·ªán v·ªõi d·ªØ li·ªáu ƒë√£ c·∫Øt (X_train_ae_input)\n",
    "ae_model = autoencoder.train_ae(ae_model, X_train_ae_input, save_path=ae_save_path)\n",
    "\n",
    "print(\"‚úÖ AE training done with L=30, N=5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96cecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Extract Latent Features Z\n",
    "print(\"üöÄ Extracting Latent Features (Z) ...\")\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a L kh·ªõp v·ªõi tham s·ªë l√∫c Train AE\n",
    "L = 30 \n",
    "\n",
    "# Tr√≠ch xu·∫•t cho t·∫≠p Train (C·∫ßn c·∫Øt l·∫•y 30 c·ªôt ƒë·∫ßu)\n",
    "X_train_latent = autoencoder.extract_features(ae_model, X_train_scaled[:, :L])\n",
    "\n",
    "# Tr√≠ch xu·∫•t cho c√°c t·∫≠p Test ƒë·ªôc l·∫≠p (C≈©ng c·∫ßn c·∫Øt l·∫•y 30 c·ªôt ƒë·∫ßu)\n",
    "X_17_test_latent = autoencoder.extract_features(ae_model, X_17_test_scaled[:, :L])\n",
    "X_18_test_latent = autoencoder.extract_features(ae_model, X_18_test_scaled[:, :L])\n",
    "X_test_all_latent = autoencoder.extract_features(ae_model, X_test_scaled[:, :L])\n",
    "\n",
    "print(f\"‚úÖ Original input shape for AE: (N, {L})\")\n",
    "print(f\"‚úÖ Latent features extracted shape: {X_train_latent.shape}\") # S·∫Ω l√† (N, 5) n·∫øu N_latent=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# 1. X√≥a c√°c bi·∫øn d·ªØ li·ªáu c≈© ƒë·ªÉ gi·∫£i ph√≥ng RAM\n",
    "print(\"üóëÔ∏è Cleaning up RAM...\")\n",
    "try:\n",
    "    del X\n",
    "    del X_train\n",
    "    del X_test\n",
    "    # N·∫øu c√≥ c√°c bi·∫øn ph·ª• kh√°c th√¨ x√≥a n·ªët\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. √âp Python thu h·ªìi b·ªô nh·ªõ ngay l·∫≠p t·ª©c\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ RAM Cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57182c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Chu·∫©n b·ªã d·ªØ li·ªáu cho Fusion\n",
    "\n",
    "# 1. L·∫•y danh s√°ch t√™n c·ªôt ƒë·∫ßy ƒë·ªß\n",
    "all_feature_names = config.SELECTED_FEATURES # List 65 c·ªôt\n",
    "\n",
    "# 2. T√¨m v·ªã tr√≠ index c·ªßa 20 c·ªôt mRMR trong list 65 c·ªôt\n",
    "try:\n",
    "    mrmr_indices = [all_feature_names.index(feat) for feat in config.mRMR_FEATURES]\n",
    "    print(f\"‚úÖ Found indices for {len(mrmr_indices)} mRMR features.\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Error: Feature name not found in SELECTED_FEATURES. Check config.py!\\nDetails: {e}\")\n",
    "    raise e\n",
    "\n",
    "# 3. C·∫Øt d·ªØ li·ªáu (Slicing) t·ª´ t·∫≠p ƒë√£ Scale\n",
    "# Ch·ªâ l·∫•y c√°c c·ªôt n·∫±m trong mrmr_indices\n",
    "print(\"‚úÇÔ∏è Slicing mRMR features from Scaled Data...\")\n",
    "X_train_mrmr = X_train_scaled[:, mrmr_indices]\n",
    "X_test_mrmr  = X_test_scaled[:, mrmr_indices]\n",
    "\n",
    "print(f\"X_train_mrmr shape: {X_train_mrmr.shape}\") # MONG ƒê·ª¢I: (N, 20)\n",
    "\n",
    "# (L∆∞u √Ω: B·∫°n ƒë√£ extract latent ·ªü Cell 6 r·ªìi, kh√¥ng c·∫ßn ch·∫°y l·∫°i d√≤ng extract ·ªü ƒë√¢y n·ªØa ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian,\n",
    "# nh∆∞ng n·∫øu b·∫°n mu·ªën ch·∫Øc ch·∫Øn th√¨ c·ª© ƒë·ªÉ nguy√™n d√≤ng extract ·ªü d∆∞·ªõi c≈©ng kh√¥ng sao)\n",
    "# print(\"üß† Extracting Latent Features...\")\n",
    "# X_train_latent = autoencoder.extract_features(ae_model, X_train_scaled)\n",
    "# X_test_latent  = autoencoder.extract_features(ae_model, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d06324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Fusion Strategy [mRMR 20] + [Latent 5]\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîó Fusing: [mRMR 20] + [Latent 5] ...\")\n",
    "\n",
    "# L·∫•y 20 ƒë·∫∑c tr∆∞ng mRMR ƒë·∫ßu ti√™n l√†m Direct Features\n",
    "mrmr_top = 20 \n",
    "\n",
    "# Fusion cho t·∫≠p Train\n",
    "X_train_fusion = np.hstack([X_train_scaled[:, :mrmr_top], X_train_latent])\n",
    "\n",
    "# Fusion cho c√°c t·∫≠p Test (S·ª≠ d·ª•ng c√°c bi·∫øn latent ƒë√£ tr√≠ch xu·∫•t ·ªü Cell 6)\n",
    "X_17_test_fusion = np.hstack([X_17_test_scaled[:, :mrmr_top], X_17_test_latent])\n",
    "X_18_test_fusion = np.hstack([X_18_test_scaled[:, :mrmr_top], X_18_test_latent])\n",
    "X_test_all_fusion = np.hstack([X_test_scaled[:, :mrmr_top], X_test_all_latent]) # ƒê·ªïi t√™n cho kh·ªõp Cell 10\n",
    "\n",
    "print(f\"üî• Fusion Completed. Shapes:\")\n",
    "print(f\"   - Train Fusion: {X_train_fusion.shape}\")\n",
    "print(f\"   - Test All Fusion: {X_test_all_fusion.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.log_experiment_details(exp_path=exp_paths['root'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train RF on Fusion Data\n",
    "print(\"üöÄ Starting RF Training on Fusion Data...\")\n",
    "\n",
    "rf_save_path = exp_paths[\"models\"] / \"rf_model.joblib\"\n",
    "\n",
    "rf_model = rf_classifier.train_rf(X_train_fusion, y_train, save_path=rf_save_path)\n",
    "\n",
    "print(\"RF training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a455af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Multi-Scenario Evaluation\n",
    "print(\"\\n--- STARTING FINAL EVALUATION ---\")\n",
    "\n",
    "test_scenarios = [\n",
    "    (X_17_test_fusion, y_17_test, \"Unseen 2017 (Legacy)\"),\n",
    "    (X_18_test_fusion, y_18_test, \"Unseen 2018 (Modern)\"),\n",
    "    (X_test_all_fusion, y_test_all, \"Global Mixed Test\")\n",
    "]\n",
    "\n",
    "for X_t, y_t, name in test_scenarios:\n",
    "    print(f\"\\nüöÄ Evaluating Scenario: {name}\")\n",
    "    evaluation.evaluate_model(\n",
    "        model=rf_model, \n",
    "        X_test=X_t, \n",
    "        y_test=y_t, \n",
    "        save_dir=exp_paths[\"figures\"], \n",
    "        dataset_name=name\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
