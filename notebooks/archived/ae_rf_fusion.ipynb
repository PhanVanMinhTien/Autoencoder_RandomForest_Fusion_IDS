{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f7d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: c:\\Users\\Admin\\Documents\\ids_ae_rf_hybrid\n",
      "Checking Data Directories:\n",
      "  - 2017: C:\\Users\\Admin\\Documents\\ids_ae_rf_hybrid\\datasets\\CIC-IDS2017 -> ‚úÖ C√≥\n",
      "  - 2018: C:\\Users\\Admin\\Documents\\ids_ae_rf_hybrid\\datasets\\CSE-CIC-IDS2018 -> ‚úÖ C√≥\n",
      "üìÇ New Experiment Created: C:\\Users\\Admin\\Documents\\ids_ae_rf_hybrid\\results\\experiments\\exp_20251217_233222\n",
      "üöÄ Experiment ID initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "#ae_rf_fusion.ipynb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Setup path\n",
    "current_dir = Path.cwd()\n",
    "root_dir = current_dir.parent\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "print(f\"Project Root: {root_dir}\")\n",
    "\n",
    "# Import modules\n",
    "from src import config, preprocessing, autoencoder, rf_classifier, evaluation, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n Data Folder thay v√¨ Data Path\n",
    "print(f\"Checking Data Directories:\")\n",
    "print(f\"  - 2017: {config.DIR_2017} -> {'‚úÖ C√≥' if config.DIR_2017.exists() else '‚ùå Kh√¥ng th·∫•y'}\")\n",
    "print(f\"  - 2018: {config.DIR_2018} -> {'‚úÖ C√≥' if config.DIR_2018.exists() else '‚ùå Kh√¥ng th·∫•y'}\")\n",
    "\n",
    "if not config.DIR_2017.exists() or not config.DIR_2018.exists():\n",
    "    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n folder trong src/config.py\")\n",
    "\n",
    "# --- KH·ªûI T·∫†O EXPERIMENT M·ªöI ---\n",
    "# B∆∞·ªõc n√†y s·∫Ω t·∫°o folder d·∫°ng: results/experiments/exp_20251212_064500/\n",
    "exp_paths = utils.setup_experiment_folder()\n",
    "print(f\"üöÄ Experiment ID initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb7b6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu load d·ªØ li·ªáu h·ªón h·ª£p (Chunking Mode)...\n",
      "üîÑ Loading mixed datasets (Chunking Mode, Binary=True)...\n",
      "‚úÖ Loaded Mixed Total: (12455891, 67)\n",
      "   Labels distribution:\n",
      "Label\n",
      "0    9150010\n",
      "1    3305881\n",
      "Name: count, dtype: int64\n",
      "‚úÖ Load xong. S·∫µn s√†ng train!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data (Mixed & Chunked)\n",
    "try:\n",
    "    print(\"üöÄ B·∫Øt ƒë·∫ßu load d·ªØ li·ªáu h·ªón h·ª£p (Chunking Mode)...\")\n",
    "    \n",
    "    # H√†m n√†y s·∫Ω t·ª± ƒë·ªông ƒë·ªçc chunk, l·ªçc c·ªôt, g·ªôp 2017+2018\n",
    "    # ƒê·∫£m b·∫£o b·∫°n ƒë√£ c·∫≠p nh·∫≠t src/preprocessing.py theo code \"Chunking\"\n",
    "    \n",
    "    X, y = preprocessing.load_mixed_datasets_chunked(binary_mode=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"‚úÖ Load xong. S·∫µn s√†ng train!\")\n",
    "except AttributeError:\n",
    "    print(\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y h√†m 'load_mixed_datasets_chunked'.\")\n",
    "    print(\"üëâ H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ c·∫≠p nh·∫≠t file src/preprocessing.py m·ªõi nh·∫•t!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2e610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è‚ôÇÔ∏è B·∫ÆT ƒê·∫¶U KI·ªÇM TRA T√çNH TO√ÄN V·∫∏N D·ªÆ LI·ªÜU...\n",
      "\n",
      "1Ô∏è‚É£  Ki·ªÉm tra Config (Khu√¥n m·∫´u):\n",
      "   - S·ªë l∆∞·ª£ng features trong Config: 67\n",
      "   - 5 features ƒë·∫ßu: ['Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets']\n",
      "   - 5 features cu·ªëi: ['Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
      "   ‚úÖ K·∫øt lu·∫≠n: Bi·∫øn X (numpy) c·ªßa b·∫°n CH·∫ÆC CH·∫ÆN tu√¢n theo th·ª© t·ª± n√†y.\n",
      "      (V√¨ code load d√πng l·ªánh: chunk[config.SELECTED_FEATURES])\n",
      "\n",
      "2Ô∏è‚É£  Ki·ªÉm tra th·ª±c t·∫ø tr√™n file CSV (M·ªói nƒÉm 1 file):\n",
      "\n",
      "   üëâ Ki·ªÉm tra dataset 2017: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "      - C·ªôt g·ªëc (Raw): [' Destination Port', ' Flow Duration', ' Total Fwd Packets'] ...\n",
      "      - ‚úÖ ƒê√£ t√¨m th·∫•y 'Destination Port' (Chu·∫©n h√≥a OK)\n",
      "      - ‚úÖ Th·ª© t·ª± c·ªôt sau khi l·ªçc kh·ªõp 100% v·ªõi Config.\n",
      "\n",
      "   üëâ Ki·ªÉm tra dataset 2018: Bot.csv\n",
      "      - C·ªôt g·ªëc (Raw): ['Dst Port', 'Protocol', 'Flow Duration'] ...\n",
      "      - ‚úÖ ƒê√£ t√¨m th·∫•y 'Destination Port' (Chu·∫©n h√≥a OK)\n",
      "      - ‚úÖ Th·ª© t·ª± c·ªôt sau khi l·ªçc kh·ªõp 100% v·ªõi Config.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Sanity Check - Ki·ªÉm tra Chu·∫©n h√≥a v√† Th·ª© t·ª±\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import config, preprocessing\n",
    "\n",
    "def verify_feature_order_and_normalization():\n",
    "    print(\"üïµÔ∏è‚ôÇÔ∏è B·∫ÆT ƒê·∫¶U KI·ªÇM TRA T√çNH TO√ÄN V·∫∏N D·ªÆ LI·ªÜU...\\n\")\n",
    "    \n",
    "    # 1. Ki·ªÉm tra Th·ª© t·ª± (Order Guarantee)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"1Ô∏è‚É£  Ki·ªÉm tra Config (Khu√¥n m·∫´u):\")\n",
    "    expected_cols = config.SELECTED_FEATURES\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng features trong Config: {len(expected_cols)}\")\n",
    "    print(f\"   - 5 features ƒë·∫ßu: {expected_cols[:5]}\")\n",
    "    print(f\"   - 5 features cu·ªëi: {expected_cols[-5:]}\")\n",
    "    print(\"   ‚úÖ K·∫øt lu·∫≠n: Bi·∫øn X (numpy) c·ªßa b·∫°n CH·∫ÆC CH·∫ÆN tu√¢n theo th·ª© t·ª± n√†y.\")\n",
    "    print(\"      (V√¨ code load d√πng l·ªánh: chunk[config.SELECTED_FEATURES])\\n\")\n",
    "\n",
    "    # 2. Ki·ªÉm tra Chu·∫©n h√≥a tr√™n File Th·ª±c (Real Data Test)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"2Ô∏è‚É£  Ki·ªÉm tra th·ª±c t·∫ø tr√™n file CSV (M·ªói nƒÉm 1 file):\")\n",
    "    \n",
    "    # L·∫•y 1 file m·∫´u t·ª´ m·ªói nƒÉm\n",
    "    files_to_check = {\n",
    "        \"2017\": next(config.DIR_2017.glob(\"*.csv\"), None),\n",
    "        \"2018\": next(config.DIR_2018.glob(\"*.csv\"), None)\n",
    "    }\n",
    "\n",
    "    for year, file_path in files_to_check.items():\n",
    "        if not file_path:\n",
    "            print(f\"   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {year}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n   üëâ Ki·ªÉm tra dataset {year}: {file_path.name}\")\n",
    "        \n",
    "        # ƒê·ªçc th√¥ 5 d√≤ng\n",
    "        df_raw = pd.read_csv(file_path, nrows=5)\n",
    "        print(f\"      - C·ªôt g·ªëc (Raw): {df_raw.columns.tolist()[:3]} ...\")\n",
    "        \n",
    "        # √Åp d·ª•ng h√†m chu·∫©n h√≥a\n",
    "        df_norm = preprocessing.normalize_column_names(df_raw)\n",
    "        \n",
    "        # Ki·ªÉm tra xem c√°c c·ªôt quan tr·ªçng c√≥ xu·∫•t hi·ªán kh√¥ng\n",
    "        # V√≠ d·ª•: Ki·ªÉm tra xem c·ªôt 'Destination Port' (ƒë√£ chu·∫©n h√≥a) c√≥ m·∫∑t kh√¥ng\n",
    "        target_col = \"Destination Port\"\n",
    "        if target_col in df_norm.columns:\n",
    "            print(f\"      - ‚úÖ ƒê√£ t√¨m th·∫•y '{target_col}' (Chu·∫©n h√≥a OK)\")\n",
    "        else:\n",
    "            print(f\"      - ‚ùå C·∫¢NH B√ÅO: Kh√¥ng th·∫•y '{target_col}'. C√≥ th·ªÉ mapping t√™n b·ªã sai!\")\n",
    "            \n",
    "        # Ki·ªÉm tra s·ª± kh·ªõp l·ªánh v·ªõi Config\n",
    "        # T·∫°o df gi·∫£ l·∫≠p b∆∞·ªõc l·ªçc\n",
    "        for col in config.SELECTED_FEATURES:\n",
    "            if col not in df_norm.columns:\n",
    "                df_norm[col] = 0.0 # Gi·∫£ l·∫≠p fill missing\n",
    "        \n",
    "        df_final = df_norm[config.SELECTED_FEATURES]\n",
    "        \n",
    "        # So s√°nh th·ª© t·ª±\n",
    "        if df_final.columns.tolist() == config.SELECTED_FEATURES:\n",
    "            print(f\"      - ‚úÖ Th·ª© t·ª± c·ªôt sau khi l·ªçc kh·ªõp 100% v·ªõi Config.\")\n",
    "        else:\n",
    "            print(f\"      - ‚ùå SAI TH·ª® T·ª∞! C·∫ßn ki·ªÉm tra l·∫°i code.\")\n",
    "\n",
    "verify_feature_order_and_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8137641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Chia t·∫≠p Train/Test & Scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=config.SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd695315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data...\n",
      "Done scaling.\n",
      "üíæ Saving scaler...\n",
      "‚úÖ Saved scaler: C:\\Users\\Admin\\Documents\\ids_ae_rf_hybrid\\results\\experiments\\exp_20251217_233222\\models\\scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Scaling & Save Scaler\n",
    "print(\"Scaling data...\")\n",
    "\n",
    "scaler = preprocessing.get_scaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test) # Quan tr·ªçng: Test ph·∫£i scale theo Train\n",
    "\n",
    "print(\"Done scaling.\")\n",
    "\n",
    "\n",
    "# --- L∆ØU SCALER V√ÄO FOLDER EXPERIMENT ---\n",
    "import joblib\n",
    "\n",
    "print(\"üíæ Saving scaler...\")\n",
    "scaler_path = exp_paths[\"models\"] / \"scaler.joblib\"  # \n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"‚úÖ Saved scaler: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3467da51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder on: cuda\n",
      "[Autoencoder] Training on cuda...\n",
      "  Epoch 5/20 - Loss: 0.170112\n",
      "  Epoch 10/20 - Loss: 0.120580\n",
      "  Epoch 15/20 - Loss: 0.098475\n",
      "  Epoch 20/20 - Loss: 0.087598\n",
      "‚úÖ AE Model saved to: C:\\Users\\Admin\\Documents\\ids_ae_rf_hybrid\\results\\experiments\\exp_20251217_233222\\models\\ae_model.pth\n",
      "AE training done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: \n",
    "print(f\"Training Autoencoder on: {config.DEVICE}\")\n",
    "\n",
    "# 1. Initialize model\n",
    "ae_model = autoencoder.DeepAutoencoder(         # hyperparameters from config.py\n",
    "    input_dim=config.AE_INPUT_DIM,              # 67\n",
    "    latent_dim=config.AE_LATENT_DIM             # 20\n",
    "    ,hidden_layers=config.AE_HIDDEN_LAYERS      # [48,32]\n",
    ")\n",
    "\n",
    "ae_save_path = exp_paths[\"models\"] / \"ae_model.pth\"\n",
    "\n",
    "# 2. Train model\n",
    "ae_model = autoencoder.train_ae(ae_model, X_train_scaled, save_path=ae_save_path)\n",
    "\n",
    "print(\"AE training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edad041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Latent Features (Z) ...\n",
      "Original shape: (9964712, 67)\n",
      "Latent shape:   (9964712, 20)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6\n",
    "# Extract Latent Features Z\n",
    "print(\"Extracting Latent Features (Z) ...\")\n",
    "X_train_latent = autoencoder.extract_features(ae_model, X_train_scaled)\n",
    "X_test_latent = autoencoder.extract_features(ae_model, X_test_scaled)\n",
    "\n",
    "print(f\"Original shape: {X_train_scaled.shape}\") # Original shape\n",
    "print(f\"Latent shape:   {X_train_latent.shape}\") # Latent shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462c3cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Cleaning up RAM...\n",
      "‚úÖ RAM Cleaned.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# 1. X√≥a c√°c bi·∫øn d·ªØ li·ªáu c≈© ƒë·ªÉ gi·∫£i ph√≥ng RAM\n",
    "print(\"üóëÔ∏è Cleaning up RAM...\")\n",
    "try:\n",
    "    del X\n",
    "    del X_train\n",
    "    del X_test\n",
    "    # N·∫øu c√≥ c√°c bi·∫øn ph·ª• kh√°c th√¨ x√≥a n·ªët\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. √âp Python thu h·ªìi b·ªô nh·ªõ ngay l·∫≠p t·ª©c\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ RAM Cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a94ff1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found indices for 20 mRMR features.\n",
      "‚úÇÔ∏è Slicing mRMR features from Scaled Data...\n",
      "X_train_mrmr shape: (9964712, 20)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Chu·∫©n b·ªã d·ªØ li·ªáu cho Fusion\n",
    "\n",
    "# 1. L·∫•y danh s√°ch t√™n c·ªôt ƒë·∫ßy ƒë·ªß\n",
    "all_feature_names = config.SELECTED_FEATURES # List 67 c·ªôt\n",
    "\n",
    "# 2. T√¨m v·ªã tr√≠ index c·ªßa 20 c·ªôt mRMR trong list 67 c·ªôt\n",
    "try:\n",
    "    mrmr_indices = [all_feature_names.index(feat) for feat in config.mRMR_FEATURES]\n",
    "    print(f\"‚úÖ Found indices for {len(mrmr_indices)} mRMR features.\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Error: Feature name not found in SELECTED_FEATURES. Check config.py!\\nDetails: {e}\")\n",
    "    raise e\n",
    "\n",
    "# 3. C·∫Øt d·ªØ li·ªáu (Slicing) t·ª´ t·∫≠p ƒë√£ Scale\n",
    "# Ch·ªâ l·∫•y c√°c c·ªôt n·∫±m trong mrmr_indices\n",
    "print(\"‚úÇÔ∏è Slicing mRMR features from Scaled Data...\")\n",
    "X_train_mrmr = X_train_scaled[:, mrmr_indices]\n",
    "X_test_mrmr  = X_test_scaled[:, mrmr_indices]\n",
    "\n",
    "print(f\"X_train_mrmr shape: {X_train_mrmr.shape}\") # MONG ƒê·ª¢I: (N, 20)\n",
    "\n",
    "# (L∆∞u √Ω: B·∫°n ƒë√£ extract latent ·ªü Cell 6 r·ªìi, kh√¥ng c·∫ßn ch·∫°y l·∫°i d√≤ng extract ·ªü ƒë√¢y n·ªØa ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian,\n",
    "# nh∆∞ng n·∫øu b·∫°n mu·ªën ch·∫Øc ch·∫Øn th√¨ c·ª© ƒë·ªÉ nguy√™n d√≤ng extract ·ªü d∆∞·ªõi c≈©ng kh√¥ng sao)\n",
    "# print(\"üß† Extracting Latent Features...\")\n",
    "# X_train_latent = autoencoder.extract_features(ae_model, X_train_scaled)\n",
    "# X_test_latent  = autoencoder.extract_features(ae_model, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55ea5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Fusing: [mRMR 20] + [Latent 20] ...\n",
      "üî• Final Fusion Input Shape: (9964712, 40)\n",
      "‚úÖ Shape chu·∫©n 40 features (20 mRMR + 20 Latent).\n",
      "‚úÖ Scaled Data removed after fusion.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Fusion \n",
    "import numpy as np\n",
    "\n",
    "print(\"üîó Fusing: [mRMR 20] + [Latent 20] ...\")\n",
    "\n",
    "# --- S·ª¨A L·ªñI ·ªû ƒê√ÇY ---\n",
    "# C≈© (Sai): np.hstack([X_train_scaled, X_train_latent]) -> Ra 87 c·ªôt\n",
    "# M·ªõi (ƒê√∫ng): np.hstack([X_train_mrmr, X_train_latent]) -> Ra 40 c·ªôt\n",
    "\n",
    "X_train_fusion = np.hstack([X_train_mrmr, X_train_latent])\n",
    "X_test_fusion  = np.hstack([X_test_mrmr, X_test_latent])\n",
    "\n",
    "print(f\"üî• Final Fusion Input Shape: {X_train_fusion.shape}\")\n",
    "# Ki·ªÉm tra nhanh: N·∫øu shape[1] != 40 th√¨ b√°o ƒë·ªông\n",
    "if X_train_fusion.shape[1] != 40:\n",
    "    print(f\"‚ö†Ô∏è WARNING: Shape kh√¥ng ph·∫£i 40! ƒêang l√† {X_train_fusion.shape[1]}\")\n",
    "else:\n",
    "    print(\"‚úÖ Shape chu·∫©n 40 features (20 mRMR + 20 Latent).\")\n",
    "\n",
    "# D·ªçn d·∫πp RAM\n",
    "try:\n",
    "    del X_train_scaled\n",
    "    del X_test_scaled\n",
    "    # X√≥a lu√¥n bi·∫øn mrmr trung gian n·∫øu mu·ªën\n",
    "    del X_train_mrmr\n",
    "    del X_test_mrmr\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ Scaled Data removed after fusion.\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e651e2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting RF Training on Fusion Data...\n",
      "[RandomForest] Training classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  8.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed: 22.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RF Model saved to: C:\\Users\\Admin\\Documents\\ids_ae_rf_hybrid\\results\\experiments\\exp_20251217_233222\\models\\rf_model.joblib\n",
      "RF training done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Train RF on Fusion Data\n",
    "print(\"üöÄ Starting RF Training on Fusion Data...\")\n",
    "\n",
    "rf_save_path = exp_paths[\"models\"] / \"rf_model.joblib\"\n",
    "\n",
    "rf_model = rf_classifier.train_rf(X_train_fusion, y_train, save_path=rf_save_path)\n",
    "\n",
    "print(\"RF training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeb060e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STARTING EVALUATION ---\n",
      "\n",
      "üìä Evaluating on Mixed Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    4.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accuracy: 0.9802\n",
      "   ‚≠ê MCC:      0.9493\n",
      "   üìù Report saved to: report_Mixed_Test_Set.txt\n",
      "   üñºÔ∏è Confusion Matrix saved to: cm_Mixed_Test_Set.png\n",
      "\n",
      "--- SUMMARY ---\n",
      "Accuracy: 0.9802\n",
      "MCC:      0.9493\n",
      "üìÇ Image saved at: C:\\Users\\Admin\\Documents\\ids_ae_rf_hybrid\\results\\experiments\\exp_20251217_233222\\figures\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Evaluate RF on Fusion Data\n",
    "\n",
    "label_names = [\"Normal\", \"Attack\"] if config.BINARY_MODE else None\n",
    "\n",
    "print(\"\\n--- STARTING EVALUATION ---\")\n",
    "\n",
    "# ƒê√°nh gi√° v√† v·∫Ω Confusion Matrix\n",
    "metrics_result = evaluation.evaluate_model(\n",
    "    model=rf_model, \n",
    "    X_test=X_test_fusion, \n",
    "    y_test=y_test, \n",
    "    save_dir=exp_paths[\"figures\"], \n",
    "    dataset_name=\"Mixed Test Set\"\n",
    ")\n",
    "\n",
    "# In t√≥m t·∫Øt k·∫øt qu·∫£\n",
    "print(\"\\n--- SUMMARY ---\")\n",
    "print(f\"Accuracy: {metrics_result['accuracy']:.4f}\")\n",
    "print(f\"MCC:      {metrics_result['mcc']:.4f}\")\n",
    "if 'recall' in metrics_result: # Ki·ªÉm tra key ƒë·ªÉ tr√°nh l·ªói\n",
    "    print(f\"Recall:   {metrics_result['recall']:.4f}\")\n",
    "print(f\"üìÇ Image saved at: {exp_paths['figures']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
