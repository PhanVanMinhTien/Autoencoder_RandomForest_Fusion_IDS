{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cf3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Path Fix\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# X√°c ƒë·ªãnh th∆∞ m·ª•c hi·ªán t·∫°i v√† th∆∞ m·ª•c g·ªëc (Root)\n",
    "current_dir = Path.cwd()\n",
    "# N·∫øu file n√†y n·∫±m trong th∆∞ m·ª•c 'notebooks', root s·∫Ω l√† cha c·ªßa n√≥ (.parent)\n",
    "# N·∫øu file n·∫±m ngay ·ªü th∆∞ m·ª•c g·ªëc, b·∫°n ch·ªâ c·∫ßn d√πng: root_dir = current_dir\n",
    "root_dir = current_dir if (current_dir / \"src\").exists() else current_dir.parent\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "print(f\"‚úÖ Project Root set to: {root_dir}\")\n",
    "\n",
    "# B√¢y gi·ªù m·ªõi import c√°c module t·ª´ src\n",
    "from src import config, preprocessing, rf_classifier, evaluation, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Kh·ªüi t·∫°o experiment m·ªõi cho Baseline\n",
    "exp_paths = utils.setup_experiment_folder()\n",
    "utils.log_experiment_details(exp_path=exp_paths['root'])\n",
    "print(f\"üöÄ Baseline Experiment Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54085f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 & 3: Sequential Loading and Splitting (Memory Optimized)\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"üöÄ Starting Memory-Optimized Loading...\")\n",
    "\n",
    "# --- 1. X·ª¨ L√ù T·∫¨P 2017 ---\n",
    "print(\"üì• Loading and Splitting 2017...\")\n",
    "X_17, y_17 = preprocessing.load_single_dataset_year('2017', binary_mode=True)\n",
    "# Ch·ªâ l·∫•y 25 ƒë·∫∑c tr∆∞ng ngay l·∫≠p t·ª©c ƒë·ªÉ ti·∫øt ki·ªám RAM\n",
    "X_17 = X_17.values[:, :25].astype('float32') \n",
    "\n",
    "X_17_train, X_17_test, y_17_train, y_17_test = train_test_split(\n",
    "    X_17, y_17, test_size=0.2, random_state=config.SEED, stratify=y_17\n",
    ")\n",
    "\n",
    "# X√≥a ngay d·ªØ li·ªáu th√¥ 2017\n",
    "del X_17, y_17\n",
    "gc.collect()\n",
    "\n",
    "# --- 2. X·ª¨ L√ù T·∫¨P 2018 ---\n",
    "print(\"üì• Loading and Splitting 2018...\")\n",
    "X_18, y_18 = preprocessing.load_single_dataset_year('2018', binary_mode=True)\n",
    "# Ch·ªâ l·∫•y 25 ƒë·∫∑c tr∆∞ng v√† √©p ki·ªÉu v·ªÅ float32\n",
    "X_18 = X_18.values[:, :25].astype('float32')\n",
    "\n",
    "X_18_train, X_18_test, y_18_train, y_18_test = train_test_split(\n",
    "    X_18, y_18, test_size=0.2, random_state=config.SEED, stratify=y_18\n",
    ")\n",
    "\n",
    "del X_18, y_18\n",
    "gc.collect()\n",
    "\n",
    "# --- 3. G·ªòP D·ªÆ LI·ªÜU (MIXED) ---\n",
    "print(\"üîó Merging into Mixed Datasets...\")\n",
    "X_train_mixed = np.vstack([X_17_train, X_18_train])\n",
    "y_train_mixed = np.concatenate([y_17_train, y_18_train])\n",
    "\n",
    "# G·ªôp t·∫≠p Test t·ªïng h·ª£p (D√≤ng b·ªã thi·∫øu g√¢y l·ªói NameError)\n",
    "X_test_all = np.vstack([X_17_test, X_18_test])\n",
    "y_test_all = np.concatenate([y_17_test, y_18_test])\n",
    "\n",
    "# X√≥a c√°c bi·∫øn trung gian sau khi g·ªôp ƒë·ªÉ gi·∫£i ph√≥ng RAM\n",
    "del X_17_train, X_18_train\n",
    "gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Mixed Train Shape: {X_train_mixed.shape}\")\n",
    "print(f\"‚úÖ Mixed Test All Shape: {X_test_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b3a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 & 5: Scaling & Final Preparation\n",
    "print(\"üîÑ Scaling baseline data (Top 25 mRMR)...\")\n",
    "scaler = preprocessing.get_scaler()\n",
    "\n",
    "# X_train_mixed l√∫c n√†y ƒë√£ l√† 25 c·ªôt t·ª´ Cell 3\n",
    "X_train_baseline = scaler.fit_transform(X_train_mixed)\n",
    "\n",
    "# Transform c√°c t·∫≠p test (ƒë√£ l√† 25 c·ªôt t·ª´ Cell 2 & 3)\n",
    "X_17_test_baseline = scaler.transform(X_17_test)\n",
    "X_18_test_baseline = scaler.transform(X_18_test)\n",
    "X_test_all_baseline = scaler.transform(X_test_all)\n",
    "\n",
    "print(f\"üî• Baseline Ready. Final Shape: {X_train_baseline.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train RF Baseline\n",
    "print(\"üöÄ Training RF Baseline (Mixed Training)...\")\n",
    "# Hu·∫•n luy·ªán tr√™n 25 ƒë·∫∑c tr∆∞ng mRMR th√¥\n",
    "rf_baseline_model = rf_classifier.train_rf(X_train_baseline, y_train_mixed)\n",
    "print(\"‚úÖ Baseline Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Final Evaluation\n",
    "print(\"üìä Evaluating RF Baseline...\")\n",
    "test_scenarios = [\n",
    "    (X_17_test_baseline, y_17_test, \"Baseline on Unseen 2017\"),\n",
    "    (X_18_test_baseline, y_18_test, \"Baseline on Unseen 2018\"),\n",
    "    (X_test_all_baseline, y_test_all, \"Baseline Global Mixed\")\n",
    "]\n",
    "\n",
    "for X_t, y_t, name in test_scenarios:\n",
    "    print(f\"\\n--- EVALUATING BASELINE: {name} ---\")\n",
    "    evaluation.evaluate_model(\n",
    "        model=rf_baseline_model, \n",
    "        X_test=X_t, \n",
    "        y_test=y_t, \n",
    "        save_dir=exp_paths[\"figures\"], \n",
    "        dataset_name=name\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
