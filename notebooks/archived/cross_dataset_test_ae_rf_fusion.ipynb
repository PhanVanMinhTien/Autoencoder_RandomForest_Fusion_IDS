{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210d6097",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Setup path\u001b[39;00m\n\u001b[0;32m     13\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(Path\u001b[38;5;241m.\u001b[39mcwd()\u001b[38;5;241m.\u001b[39mparent))\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, preprocessing, autoencoder, rf_classifier, evaluation, utils\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# T·∫°o folder l∆∞u k·∫øt qu·∫£ ri√™ng cho Cross-Test\u001b[39;00m\n\u001b[0;32m     17\u001b[0m exp_paths \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39msetup_experiment_folder()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Setup path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src import config, preprocessing, autoencoder, rf_classifier, evaluation, utils\n",
    "\n",
    "# T·∫°o folder l∆∞u k·∫øt qu·∫£ ri√™ng cho Cross-Test\n",
    "exp_paths = utils.setup_experiment_folder()\n",
    "print(f\"üìÇ Saving results to: {exp_paths['root']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# K·ªäCH B·∫¢N 1: TRAIN TR√äN 2017 -> TEST TR√äN 2018\n",
    "# ==============================================================================\n",
    "\n",
    "# Cell 2: Load Data 2017 (TRAIN)\n",
    "print(\"\\n--- 1. LOADING TRAIN SET (CIC-IDS2017) ---\")\n",
    "# H√†m n√†y tr·∫£ v·ªÅ DataFrame, ta gi·ªØ nguy√™n ƒë·ªÉ d·ªÖ ki·ªÉm so√°t l√∫c ƒë·∫ßu\n",
    "df_train, y_train = preprocessing.load_single_dataset_year('2017', binary_mode=True)\n",
    "\n",
    "# Chuy·ªÉn sang numpy ƒë·ªÉ ƒë∆∞a v√†o model\n",
    "X_train = df_train.values\n",
    "\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Label Distribution: {pd.Series(y_train).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac8c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Scaling (Fit on Train)\n",
    "print(\"\\n--- 2. SCALING ---\")\n",
    "scaler = preprocessing.get_scaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "print(\"‚úÖ Scaler fitted on 2017.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a252ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Train Autoencoder on 2017\n",
    "print(\"\\n--- 3. TRAIN AUTOENCODER ---\")\n",
    "ae_model = autoencoder.DeepAutoencoder(\n",
    "    input_dim=config.AE_INPUT_DIM, \n",
    "    latent_dim=config.AE_LATENT_DIM,\n",
    "    hidden_layers=config.AE_HIDDEN_LAYERS\n",
    ")\n",
    "# Train AE\n",
    "# M·ªõi (L∆∞u v√†o folder experiment hi·ªán t·∫°i)\n",
    "ae_save_path = exp_paths['models'] / \"ae_model_2017.pth\"\n",
    "\n",
    "ae_model = autoencoder.train_ae(ae_model, X_train_scaled, save_path=ae_save_path)\n",
    "\n",
    "# Extract Latent Features cho Train\n",
    "print(\"Extracting latent features for Train set...\")\n",
    "X_train_latent = autoencoder.extract_features(ae_model, X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Prepare Fusion Data for Train\n",
    "print(\"\\n--- 4. PREPARE FUSION DATA (TRAIN) ---\")\n",
    "# L·∫•y mRMR features\n",
    "all_feature_names = config.SELECTED_FEATURES\n",
    "mrmr_indices = [all_feature_names.index(feat) for feat in config.mRMR_FEATURES]\n",
    "\n",
    "X_train_mrmr = X_train_scaled[:, mrmr_indices]\n",
    "X_train_fusion = np.hstack([X_train_mrmr, X_train_latent])\n",
    "\n",
    "print(f\"Fusion Train Shape: {X_train_fusion.shape}\")\n",
    "\n",
    "# X√≥a b·ªõt bi·∫øn n·∫∑ng ƒë·ªÉ gi·∫£i ph√≥ng RAM cho t·∫≠p Test\n",
    "del X_train, X_train_scaled, df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train Random Forest on 2017\n",
    "print(\"\\n--- 5. TRAIN CLASSIFIER (RF) ---\")\n",
    "rf_save_path = exp_paths['models'] / \"rf_model_2017.joblib\"\n",
    "rf_model = rf_classifier.train_rf(X_train_fusion, y_train, save_path=rf_save_path)\n",
    "print(\"‚úÖ RF Model Trained on 2017.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a54c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CHUY·ªÇN SANG GIAI ƒêO·∫†N TEST TR√äN 2018\n",
    "# ==============================================================================\n",
    "\n",
    "# Cell 7: Load Data 2018 (TEST)\n",
    "print(\"\\n--- 6. LOADING TEST SET (CSE-CIC-IDS2018) ---\")\n",
    "df_test, y_test = preprocessing.load_single_dataset_year('2018', binary_mode=True)\n",
    "X_test = df_test.values\n",
    "\n",
    "print(f\"Test Data Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Process Test Data (D√πng Scaler & AE c≈©)\n",
    "print(\"\\n--- 7. PROCESSING TEST DATA ---\")\n",
    "\n",
    "# A. Scale (D√πng scaler c·ªßa 2017)\n",
    "print(\"Scaling using 2017 scaler...\")\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# B. Extract Latent (D√πng AE c·ªßa 2017)\n",
    "print(\"Extracting latent features...\")\n",
    "X_test_latent = autoencoder.extract_features(ae_model, X_test_scaled)\n",
    "\n",
    "# C. Slice mRMR Features\n",
    "print(\"Slicing mRMR features...\")\n",
    "X_test_mrmr = X_test_scaled[:, mrmr_indices]\n",
    "\n",
    "# D. Fusion\n",
    "print(\"Fusing...\")\n",
    "X_test_fusion = np.hstack([X_test_mrmr, X_test_latent])\n",
    "print(f\"Fusion Test Shape: {X_test_fusion.shape}\")\n",
    "\n",
    "# Clean RAM\n",
    "del X_test, X_test_scaled, df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluate Cross-Dataset\n",
    "print(\"\\n--- 8. FINAL EVALUATION (Train: 2017 -> Test: 2018) ---\")\n",
    "metrics = evaluation.evaluate_model(\n",
    "    rf_model, \n",
    "    X_test_fusion, \n",
    "    y_test, \n",
    "    save_dir=exp_paths['figures'],\n",
    "    dataset_name=\"Cross-Test (Train 17 - Test 18)\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== K·∫æT QU·∫¢ ===\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"MCC:      {metrics['mcc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d507dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BATTLE: HYBRID MODEL (AE+RF) vs. PURE RF (BASELINE)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nü•ä B·∫ÆT ƒê·∫¶U SO S√ÅNH: PROPOSED METHOD vs. BASELINE ü•ä\")\n",
    "\n",
    "# --- 1. Train Baseline (Pure RF v·ªõi mRMR Features) ---\n",
    "print(\"üëâ Training Baseline RF (Only mRMR 20 features)...\")\n",
    "# X_train_mrmr ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü b∆∞·ªõc tr∆∞·ªõc (c·∫Øt t·ª´ X_train_scaled)\n",
    "# L∆∞u √Ω: C·∫ßn ƒë·∫£m b·∫£o bi·∫øn X_train_mrmr c√≤n trong RAM. \n",
    "# N·∫øu l·ª° x√≥a r·ªìi th√¨ ph·∫£i c·∫Øt l·∫°i t·ª´ X_train_scaled (n·∫øu X_train_scaled c≈©ng x√≥a th√¨ ph·∫£i load l·∫°i 2017).\n",
    "# Gi·∫£ s·ª≠ b·∫°n ch·∫°y li·ªÅn m·∫°ch th√¨ c√°c bi·∫øn n√†y v·∫´n c√≤n ho·∫∑c c√≥ th·ªÉ t·∫°o l·∫°i d·ªÖ d√†ng.\n",
    "\n",
    "# C√°ch an to√†n nh·∫•t ƒë·ªÉ t·∫°o l·∫°i d·ªØ li·ªáu train cho Baseline (n·∫øu l·ª° x√≥a RAM):\n",
    "# (Ch·ªâ ch·∫°y ƒëo·∫°n reload n√†y n·∫øu b·∫°n ƒë√£ l·ª° del bi·∫øn X_train_scaled)\n",
    "# df_train_temp, y_train_temp = preprocessing.load_single_dataset_year('2017', binary_mode=True)\n",
    "# scaler_temp = preprocessing.get_scaler()\n",
    "# X_train_sc_temp = scaler_temp.fit_transform(df_train_temp.values)\n",
    "# mrmr_indices = [config.SELECTED_FEATURES.index(f) for f in config.MRMR_FEATURES]\n",
    "# X_train_mrmr = X_train_sc_temp[:, mrmr_indices]\n",
    "# y_train = y_train_temp\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "rf_baseline = rf_classifier.train_rf(\n",
    "    X_train_mrmr, \n",
    "    y_train, \n",
    "    save_path=exp_paths['models'] / \"rf_baseline_2017.joblib\"  # <-- L∆∞u v√†o folder exp hi·ªán t·∫°i\n",
    ") \n",
    "\n",
    "# --- 2. Evaluate Baseline ---\n",
    "print(\"Evaluating Baseline on Test set (2018)...\")\n",
    "# T∆∞∆°ng t·ª±, ƒë·∫£m b·∫£o X_test_mrmr c√≥ s·∫µn\n",
    "baseline_metrics = evaluation.evaluate_model(\n",
    "    rf_baseline, \n",
    "    X_test_mrmr, \n",
    "    y_test, \n",
    "    save_dir=None,\n",
    "    dataset_name=\"Baseline (Pure RF)\"\n",
    ")\n",
    "\n",
    "# --- 3. So s√°nh tr·ª±c ti·∫øp ---\n",
    "# L·∫•y k·∫øt qu·∫£ t·ª´ metrics c≈© (b·∫°n v·ª´a ch·∫°y xong)\n",
    "# L∆∞u √Ω: Bi·∫øn 'metrics' ch·ª©a k·∫øt qu·∫£ c·ªßa Hybrid model v·ª´a ch·∫°y ·ªü tr√™n\n",
    "hybrid_acc = metrics['accuracy'] \n",
    "baseline_acc = baseline_metrics['accuracy']\n",
    "\n",
    "hybrid_mcc = metrics['mcc']\n",
    "baseline_mcc = baseline_metrics['mcc']\n",
    "\n",
    "# L·∫•y th√™m Recall Attack ƒë·ªÉ so s√°nh (Quan tr·ªçng nh·∫•t)\n",
    "hybrid_recall = metrics['report']['Attack']['recall']\n",
    "baseline_recall = baseline_metrics['report']['Attack']['recall']\n",
    "\n",
    "print(\"\\nüìä === B·∫¢NG K·∫æT QU·∫¢ ƒê·ªêI ƒê·∫¶U (Train 17 -> Test 18) ===\")\n",
    "print(f\"{'Metric':<20} | {'Baseline (Pure RF)':<20} | {'Hybrid (Proposed)':<20} | {'Improvement':<15}\")\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'Accuracy':<20} | {baseline_acc:.4f}{'':<14} | {hybrid_acc:.4f}{'':<14} | {hybrid_acc - baseline_acc:+.4f}\")\n",
    "print(f\"{'MCC':<20} | {baseline_mcc:.4f}{'':<14} | {hybrid_mcc:.4f}{'':<14} | {hybrid_mcc - baseline_mcc:+.4f}\")\n",
    "print(f\"{'Recall (Attack)':<20} | {baseline_recall:.4f}{'':<14} | {hybrid_recall:.4f}{'':<14} | {hybrid_recall - baseline_recall:+.4f}\")\n",
    "\n",
    "# --- 4. V·∫Ω bi·ªÉu ƒë·ªì ---\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "metrics_names = ['Accuracy', 'MCC', 'Recall (Attack)']\n",
    "baseline_scores = [baseline_acc, baseline_mcc, baseline_recall]\n",
    "hybrid_scores = [hybrid_acc, hybrid_mcc, hybrid_recall]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, baseline_scores, width, label='Baseline (Pure RF)', color='gray')\n",
    "rects2 = ax.bar(x + width/2, hybrid_scores, width, label='Hybrid (AE+RF)', color='royalblue')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Cross-Dataset Comparison: Pure RF vs Hybrid')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_names)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(exp_paths['figures'] / \"cross_dataset_comparison.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
