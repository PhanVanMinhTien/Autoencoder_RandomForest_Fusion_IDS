# IDS AE+RF Fusion - Network Intrusion Detection System

## ğŸ“‹ MÃ´ Táº£ Dá»± Ãn

Dá»± Ã¡n nÃ y phÃ¡t triá»ƒn má»™t **há»‡ thá»‘ng phÃ¡t hiá»‡n xÃ¢m nháº­p máº¡ng (IDS)** sá»­ dá»¥ng ká»¹ thuáº­t **káº¿t há»£p Autoencoder (AE) vÃ  Random Forest (RF)** Ä‘á»ƒ phÃ¡t hiá»‡n vÃ  phÃ¢n loáº¡i cÃ¡c cuá»™c táº¥n cÃ´ng máº¡ng.

### Má»¥c tiÃªu chÃ­nh:
- **Giáº£m chiá»u dá»¯ liá»‡u**: Sá»­ dá»¥ng Autoencoder Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng potent tá»« dá»¯ liá»‡u lÆ°u lÆ°á»£ng máº¡ng
- **PhÃ¢n loáº¡i**: Sá»­ dá»¥ng Random Forest Ä‘á»ƒ phÃ¢n loáº¡i dá»¯ liá»‡u thÃ nh "BÃ¬nh thÆ°á»ng" (Benign) hoáº·c "Táº¥n cÃ´ng" (Attack)
- **So sÃ¡nh phÆ°Æ¡ng phÃ¡p**: ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t cá»§a AE+RF so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ (RF-only, SVM)
- **Kiá»ƒm tra cross-dataset**: ÄÃ¡nh giÃ¡ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a trÃªn cÃ¡c bá»™ dá»¯ liá»‡u khÃ¡c nhau

---

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
ids_ae_rf_fusion/
â”œâ”€â”€ README.md                           # TÃ i liá»‡u nÃ y
â”œâ”€â”€ requirements.txt                    # Danh sÃ¡ch thÆ° viá»‡n Python
â”œâ”€â”€ setup_new_env.py                    # Script khá»Ÿi táº¡o mÃ´i trÆ°á»ng
â”‚
â”œâ”€â”€ src/                                # MÃ£ nguá»“n chÃ­nh
â”‚   â”œâ”€â”€ config.py                       # Cáº¥u hÃ¬nh dá»± Ã¡n (Ä‘Æ°á»ng dáº«n, siÃªu tham sá»‘)
â”‚   â”œâ”€â”€ autoencoder.py                  # Model Deep Autoencoder (PyTorch)
â”‚   â”œâ”€â”€ rf_classifier.py                # Random Forest Classifier
â”‚   â”œâ”€â”€ preprocessing.py                # Tiá»n xá»­ lÃ½ vÃ  lÃ m sáº¡ch dá»¯ liá»‡u
â”‚   â”œâ”€â”€ feature_selection.py            # Lá»±a chá»n Ä‘áº·c trÆ°ng (mRMR)
â”‚   â”œâ”€â”€ evaluation.py                   # ÄÃ¡nh giÃ¡ model & visualize káº¿t quáº£
â”‚   â””â”€â”€ utils.py                        # CÃ¡c hÃ m tiá»‡n Ã­ch
â”‚
â”œâ”€â”€ datasets/                           # Dá»¯ liá»‡u (chá»‰ chá»©a metadata)
â”‚   â””â”€â”€ dataset.txt                     # MÃ´ táº£ bá»™ dá»¯ liá»‡u
â”‚
â”œâ”€â”€ notebooks/                          # Jupyter Notebooks (pipeline thá»±c nghiá»‡m)
â”‚   â”œâ”€â”€ 0a_mRMR_selection.ipynb         # Lá»±a chá»n Ä‘áº·c trÆ°ng (mRMR)
â”‚   â”œâ”€â”€ 0b_mRMR_features_and_latent_features.ipynb
â”‚   â”‚
â”‚   â”œâ”€â”€ 1a_ae_rf_fusion_mix_training.ipynb      # [Giai Ä‘oáº¡n 1] Huáº¥n luyá»‡n há»—n há»£p
â”‚   â”œâ”€â”€ 1b_rf_mix_training.ipynb                # Baseline: RF-only
â”‚   â”œâ”€â”€ 1c_svm_mix_training.ipynb               # Baseline: SVM
â”‚   â”‚
â”‚   â”œâ”€â”€ 2a_ae_rf_fusion_within_dataset.ipynb    # [Giai Ä‘oáº¡n 2] Kiá»ƒm tra trong-dataset
â”‚   â”œâ”€â”€ 2b_rf_within_dataset.ipynb
â”‚   â”œâ”€â”€ 2c_svm_within_dataset.ipynb
â”‚   â”‚
â”‚   â”œâ”€â”€ 3a_ae_rf_fusion_cross_dataset.ipynb     # [Giai Ä‘oáº¡n 3] Kiá»ƒm tra cross-dataset
â”‚   â”œâ”€â”€ 3b_rf_cross_dataset.ipynb
â”‚   â”œâ”€â”€ 3c_svm_cross_dataset.ipynb
â”‚   â”‚
â”‚   â””â”€â”€ archived/                       # CÃ¡c notebook cÅ© & thá»­ nghiá»‡m
â”‚
â”œâ”€â”€ results/                            # Káº¿t quáº£ thá»±c nghiá»‡m
â”‚   â”œâ”€â”€ experiments/                    # ThÆ° má»¥c lÆ°u káº¿t quáº£ tá»«ng láº§n cháº¡y
â”‚   â”‚   â””â”€â”€ exp_YYYYMMDD_HHMMSS/
â”‚   â”‚       â”œâ”€â”€ report_*.txt            # BÃ¡o cÃ¡o chi tiáº¿t (Accuracy, MCC, F1, ...)
â”‚   â”‚       â”œâ”€â”€ figures/                # Confusion Matrix & visualization
â”‚   â”‚       â”œâ”€â”€ models/                 # MÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
â”‚   â”‚       â””â”€â”€ experiment_details.txt  # Cáº¥u hÃ¬nh & siÃªu tham sá»‘
â”‚   â”‚
â”‚   â””â”€â”€ Summary/                        # TÃ³m táº¯t so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p
â”‚
â””â”€â”€ models/                             # (TÃ¹y chá»n) LÆ°u cÃ¡c mÃ´ hÃ¬nh huáº¥n luyá»‡n
```

---

## ğŸ› ï¸ YÃªu Cáº§u & CÃ i Äáº·t

### 1. **YÃªu Cáº§u Há»‡ Thá»‘ng**
- Python 3.8+
- CUDA (tÃ¹y chá»n, Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ GPU náº¿u cÃ³)
- RAM: 8GB+ (khuyáº¿n nghá»‹ 16GB+)

### 2. **CÃ i Äáº·t ThÆ° Viá»‡n**

```bash
# CÃ¡ch 1: CÃ i Ä‘áº·t trá»±c tiáº¿p tá»« requirements.txt
pip install -r requirements.txt

# CÃ¡ch 2: Sá»­ dá»¥ng script setup (náº¿u cÃ³)
python setup_new_env.py
```

### 3. **Danh SÃ¡ch ThÆ° Viá»‡n**

CÃ¡c thÆ° viá»‡n chÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng:
- **numpy**, **pandas** - Xá»­ lÃ½ dá»¯ liá»‡u
- **scikit-learn** - Machine Learning (Random Forest, SVM, metrics)
- **torch** - Deep Learning (Autoencoder)
- **matplotlib**, **seaborn** - Visualize káº¿t quáº£
- **joblib** - LÆ°u & táº£i mÃ´ hÃ¬nh
- **mrmr-selection** - Lá»±a chá»n Ä‘áº·c trÆ°ng

---

## ğŸ“Š Bá»™ Dá»¯ Liá»‡u

Dá»± Ã¡n sá»­ dá»¥ng hai bá»™ dá»¯ liá»‡u IDS cÃ´ng cá»™ng:

| Bá»™ Dá»¯ Liá»‡u | NÄƒm | Sá»‘ máº«u | Äáº·c Ä‘iá»ƒm |
|-----------|-----|--------|---------|
| **CIC-IDS2017** | 2017 | ~2.8M | LÆ°u lÆ°á»£ng thá»±c tá»« máº¡ng, 15 loáº¡i táº¥n cÃ´ng |
| **CSE-CIC-IDS2018** | 2018 | ~2.5M | Cáº­p nháº­t tá»« 2017, cÃ¡c táº¥n cÃ´ng hiá»‡n Ä‘áº¡i hÆ¡n |

### CÃ¡c Loáº¡i Táº¥n CÃ´ng:
- **BENIGN** - LÆ°u lÆ°á»£ng bÃ¬nh thÆ°á»ng
- **DOS** - Denial of Service (HULK, GoldenEye, SlowLoris, SlowHTTPTest)
- **DDOS** - Distributed DoS (HOIC, LOIC)
- **BRUTEFORCE** - Táº¥n cÃ´ng brute force (FTP, SSH)
- **BOT** - Botnet
- **PORTSCAN** - QuÃ©t cá»•ng
- **WEB** - Táº¥n cÃ´ng Web (SQL Injection, XSS, Brute Force)
- **INFILTRATION** - ThÃ¢m nháº­p
- **HEARTBLEED** - Lá»— há»•ng Heartbleed

---

## ğŸ”§ MÃ´ Táº£ CÃ¡c Module ChÃ­nh

### 1. **config.py** - Cáº¥u HÃ¬nh ToÃ n Dá»± Ãn

Quáº£n lÃ½ táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh:
- **ÄÆ°á»ng dáº«n dá»¯ liá»‡u**: ThÆ° má»¥c datasets, results
- **SiÃªu tham sá»‘ dá»¯ liá»‡u**:
  - `BINARY_MODE = True` - Cháº¿ Ä‘á»™ phÃ¢n loáº¡i nhá»‹ phÃ¢n (Benign vs Attack)
  - `CHUNK_SIZE = 100000` - KÃ­ch thÆ°á»›c Ä‘á»c file má»—i láº§n
  - `SEED = 42` - Seed cho reproducibility

- **Tiá»n xá»­ lÃ½**:
  - `DROP_COLS` - CÃ¡c cá»™t loáº¡i bá» (identifier, sparse, mismatch)
  - `RENAME_2018_TO_2017` - Map tÃªn cá»™t 2018 â†’ 2017 (chuáº©n hÃ³a)
  - `BENIGN_LABELS` - NhÃ£n Ä‘Æ°á»£c coi lÃ  "bÃ¬nh thÆ°á»ng"

- **SiÃªu tham sá»‘ Autoencoder**:
  - Input dimension, latent dimension, hidden layers

- **SiÃªu tham sá»‘ Random Forest**:
  - Sá»‘ lÆ°á»£ng cÃ¢y (n_estimators)
  - Äá»™ sÃ¢u tá»‘i Ä‘a (max_depth)
  - CÃ¢n báº±ng class weight

### 2. **autoencoder.py** - Deep Autoencoder

```python
class DeepAutoencoder(nn.Module):
    """
    Autoencoder Ä‘á»‘i xá»©ng vá»›i:
    - Encoder: Input â†’ Hidden Layers â†’ Latent (bottleneck)
    - Decoder: Latent â†’ Hidden Layers â†’ Output
    
    Äáº·c Ä‘iá»ƒm:
    - Batch Normalization + LeakyReLU
    - Output tuyáº¿n tÃ­nh (linear activation)
    - DÃ¹ng MSE Loss Ä‘á»ƒ tÃ¡i táº¡o dá»¯ liá»‡u
    """
```

**Chá»©c nÄƒng**:
- TÃ¡i táº¡o (Reconstruction): Há»c trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u bÃ¬nh thÆ°á»ng
- Giáº£m chiá»u: NÃ©n 67 features xuá»‘ng latent space nhá» hÆ¡n
- PhÃ¡t hiá»‡n: CÃ¡c táº¥n cÃ´ng cÃ³ lá»—i tÃ¡i táº¡o cao (anomaly detection)

### 3. **rf_classifier.py** - Random Forest Classifier

```python
def train_rf(X_train, y_train, save_path=None):
    """
    Huáº¥n luyá»‡n Random Forest vá»›i:
    - n_estimators = 200
    - max_depth = 20
    - class_weight = 'balanced' (xá»­ lÃ½ máº¥t cÃ¢n báº±ng dá»¯ liá»‡u)
    - n_jobs = 8 (Ä‘a xá»­ lÃ½)
    """
```

### 4. **preprocessing.py** - Tiá»n Xá»­ LÃ½

- Äá»c dá»¯ liá»‡u theo chunks
- LÃ m sáº¡ch dá»¯ liá»‡u (loáº¡i bá» NaN, duplicates)
- Chuáº©n hÃ³a cá»™t tÃªn (2018 â†’ 2017)
- Loáº¡i bá» cá»™t identifier (Flow ID, IP, Timestamp)
- Chuáº©n hÃ³a dá»¯ liá»‡u (StandardScaler)
- MÃ£ hÃ³a nhÃ£n (Benign â†’ 0, Attack â†’ 1)

### 5. **feature_selection.py** - Lá»±a Chá»n Äáº·c TrÆ°ng

Sá»­ dá»¥ng **mRMR (Minimum Redundancy Maximum Relevance)**:
- Chá»n Ä‘áº·c trÆ°ng cÃ³ tÆ°Æ¡ng quan cao vá»›i nhÃ£n
- Æ¯u tiÃªn Ä‘áº·c trÆ°ng Ã­t dá»± phÃ²ng

### 6. **evaluation.py** - ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh

CÃ¡c metrics Ä‘Æ°á»£c tÃ­nh toÃ¡n:
- **Accuracy** - Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ
- **MCC (Matthews Correlation Coefficient)** - Metric cÃ¢n báº±ng cho dá»¯ liá»‡u máº¥t cÃ¢n báº±ng
- **Precision, Recall, F1-score** - Chi tiáº¿t tá»«ng class
- **Confusion Matrix** - Visualize true/false positives

---

## ğŸš€ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### **Giai Ä‘oáº¡n 0: Chuáº©n Bá»‹ Dá»¯ Liá»‡u**
1. Táº£i CIC-IDS2017 & CSE-CIC-IDS2018 vÃ o `datasets/CIC-IDS2017` vÃ  `datasets/CSE-CIC-IDS2018`
2. Cháº¡y notebook `0a_mRMR_selection.ipynb` Ä‘á»ƒ lá»±a chá»n Ä‘áº·c trÆ°ng
3. Káº¿t quáº£: Danh sÃ¡ch ~20-30 Ä‘áº·c trÆ°ng tá»‘t nháº¥t

### **Giai Ä‘oáº¡n 1: Huáº¥n Luyá»‡n Há»—n Há»£p (Mixed Training)**
- Gá»™p dá»¯ liá»‡u tá»« cáº£ 2017 vÃ  2018
- Huáº¥n luyá»‡n: 80% / Kiá»ƒm tra: 20%
- Cháº¡y:
  - `1a_ae_rf_fusion_mix_training.ipynb` - **PhÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t**
  - `1b_rf_mix_training.ipynb` - Baseline RF-only
  - `1c_svm_mix_training.ipynb` - Baseline SVM
- Káº¿t quáº£: So sÃ¡nh 3 phÆ°Æ¡ng phÃ¡p

### **Giai Ä‘oáº¡n 2: Kiá»ƒm Tra Trong-Dataset (Within-Dataset)**
- Huáº¥n luyá»‡n & kiá»ƒm tra trÃªn cÃ¹ng bá»™ dá»¯ liá»‡u:
  - 2017 train/test
  - 2018 train/test
- Cháº¡y:
  - `2a_ae_rf_fusion_within_dataset.ipynb`
  - `2b_rf_within_dataset.ipynb`
  - `2c_svm_within_dataset.ipynb`

### **Giai Ä‘oáº¡n 3: Kiá»ƒm Tra Cross-Dataset (Cross-Dataset)**
- Huáº¥n luyá»‡n trÃªn 1 bá»™, kiá»ƒm tra trÃªn bá»™ khÃ¡c:
  - Train 2017 â†’ Test 2018
  - Train 2018 â†’ Test 2017
- Cháº¡y:
  - `3a_ae_rf_fusion_cross_dataset.ipynb`
  - `3b_rf_cross_dataset.ipynb`
  - `3c_svm_cross_dataset.ipynb`
- **Má»¥c Ä‘Ã­ch**: ÄÃ¡nh giÃ¡ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a

---

## ğŸ“ˆ Káº¿t Quáº£ Dá»± Kiáº¿n

### Hiá»‡u Suáº¥t Dá»± Kiáº¿n
| PhÆ°Æ¡ng PhÃ¡p | Mixed | Within-2017 | Within-2018 | Cross (17â†’18) | Cross (18â†’17) |
|------------|-------|-------------|-------------|---------------|---------------|
| **AE+RF (Äá» xuáº¥t)** | ~95-98% | ~97-99% | ~95-97% | ~85-90% | ~82-87% |
| **RF-only (Baseline)** | ~92-95% | ~95-98% | ~93-96% | ~80-85% | ~78-83% |
| **SVM (Baseline)** | ~90-94% | ~93-97% | ~91-95% | ~75-82% | ~73-80% |

**Ghi chÃº**: Cross-dataset thÆ°á»ng cÃ³ káº¿t quáº£ tháº¥p hÆ¡n do sá»± khÃ¡c biá»‡t distribution giá»¯a cÃ¡c bá»™ dá»¯ liá»‡u

---

## ğŸ“ CÃ¡ch Cháº¡y Má»™t Notebook

### 1. **Tá»« Command Line**
```bash
# Cháº¡y notebook vÃ  lÆ°u káº¿t quáº£
jupyter nbconvert --to notebook --execute --output output.ipynb 1a_ae_rf_fusion_mix_training.ipynb

# Hoáº·c má»Ÿ Jupyter Lab Ä‘á»ƒ cháº¡y tÆ°Æ¡ng tÃ¡c
jupyter lab
```

### 2. **Trong VS Code**
- Má»Ÿ file `.ipynb`
- Chá»n kernel Python
- Cháº¡y tá»«ng cell hoáº·c Run All

### 3. **Thay Ä‘á»•i Cáº¥u HÃ¬nh**
- Chá»‰nh sá»­a `src/config.py`:
  - ÄÆ°á»ng dáº«n dá»¯ liá»‡u
  - SiÃªu tham sá»‘ mÃ´ hÃ¬nh
  - Feature selection parameters
- Cáº¥u hÃ¬nh tá»± Ä‘á»™ng Ä‘Æ°á»£c load trong notebooks

---

## ğŸ” Cáº¥u TrÃºc Káº¿t Quáº£ (Results)

Má»—i láº§n cháº¡y notebook táº¡o thÆ° má»¥c káº¿t quáº£:
```
results/experiments/exp_YYYYMMDD_HHMMSS/
â”œâ”€â”€ experiment_details.txt          # Cáº¥u hÃ¬nh, siÃªu tham sá»‘
â”œâ”€â”€ report_Mixed_Test_Set.txt       # BÃ¡o cÃ¡o metrics chi tiáº¿t
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ cm_Mixed_Test_Set.png       # Confusion Matrix
â”‚   â””â”€â”€ ... (cÃ¡c hÃ¬nh khÃ¡c)
â””â”€â”€ models/
    â””â”€â”€ model.joblib / model.pt     # MÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
```

---

## ğŸ¯ Nhá»¯ng Äiá»ƒm ChÃ­nh

âœ… **Æ¯u Ä‘iá»ƒm cá»§a AE+RF Fusion**:
- Giáº£m chiá»u: Loáº¡i bá» features noise & redundant
- Há»c Ä‘Æ°á»£c Ä‘áº·c trÆ°ng tá»± Ä‘á»™ng tá»« dá»¯ liá»‡u
- RF nhanh & khÃ´ng cáº§n Ä‘iá»u chá»‰nh siÃªu tham sá»‘ phá»©c táº¡p
- Káº¿t há»£p: Lá»£i tháº¿ cá»§a cáº£ 2 phÆ°Æ¡ng phÃ¡p

âš ï¸ **ThÃ¡ch thá»©c**:
- Cross-dataset performance: MÃ´ hÃ¬nh khÃ³ tá»•ng quÃ¡t hÃ³a giá»¯a cÃ¡c nÄƒm
- Class imbalance: Dá»¯ liá»‡u bÃ¬nh thÆ°á»ng nhiá»u hÆ¡n dá»¯ liá»‡u táº¥n cÃ´ng
- Hyperparameter tuning: Cáº§n tÃ¬m optimal latent dimension

---

## ğŸ¤ ÄÃ³ng GÃ³p & Há»— Trá»£

Náº¿u cÃ³ cÃ¢u há»i hoáº·c muá»‘n cáº£i tiáº¿n dá»± Ã¡n:
1. Kiá»ƒm tra `notebooks/archived/` Ä‘á»ƒ xem cÃ¡c thá»­ nghiá»‡m trÆ°á»›c Ä‘Ã³
2. Tham kháº£o cÃ¡c bÃ¡o cÃ¡o trong `results/experiments/`
3. Chá»‰nh sá»­a `src/config.py` Ä‘á»ƒ thá»­ cÃ¡c siÃªu tham sá»‘ khÃ¡c nhau

---

## ğŸ“š Tham Kháº£o

- **CIC-IDS2017**: https://www.unb.ca/cic/datasets/ids-2017.html
- **CSE-CIC-IDS2018**: https://www.unb.ca/cic/datasets/ids-2018.html
- **mRMR Feature Selection**: https://github.com/ELELAB/pymrmr
- **PyTorch Autoencoder**: https://pytorch.org/tutorials/beginner/introyt/autoencoders_tutorial.html
- **scikit-learn Random Forest**: https://scikit-learn.org/stable/modules/ensemble.html#random-forests

---

**NgÃ y táº¡o**: January 2026  
**PhiÃªn báº£n**: 1.0
