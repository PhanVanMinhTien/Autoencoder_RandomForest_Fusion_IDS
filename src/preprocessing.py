# src/preprocessing.py
import pandas as pd
import numpy as np
import glob
from sklearn.preprocessing import StandardScaler
from . import config

def normalize_column_names(df):
    """
    Chu·∫©n h√≥a t√™n c·ªôt:
    1. X√≥a kho·∫£ng tr·∫Øng (Strip)
    2. ƒê·ªïi t√™n c·ªôt 2018 -> 2017 (d·ª±a tr√™n config)
    3. X√≥a c√°c c·ªôt trong DROP_COLS (Identifier, Sparse, Duplicate)
    """
    # 1. Strip whitespace
    df.columns = df.columns.str.strip()
    
    # 2. Rename (2018 -> 2017)
    # B∆∞·ªõc n√†y c·ª±c k·ª≥ quan tr·ªçng cho Cross-dataset test
    df = df.rename(columns=config.RENAME_2018_TO_2017)
    
    # 3. Drop Columns (n·∫øu t·ªìn t·∫°i)
    cols_to_drop = [c for c in config.DROP_COLS if c in df.columns]
    if cols_to_drop:
        df.drop(columns=cols_to_drop, inplace=True)
        
    return df

def normalize_labels(series, binary_mode=True):
    """
    Chu·∫©n h√≥a nh√£n t·ª´ Raw -> Canonical -> Binary/Multi
    Tham s·ªë:
        series: pd.Series ch·ª©a nh√£n g·ªëc
        binary_mode: True (Default) -> Tr·∫£ v·ªÅ 0/1. False -> Tr·∫£ v·ªÅ t√™n nh√£n Canonical.
    """
    # 1. Raw -> Canonical (Chu·∫©n h√≥a t√™n t·∫•n c√¥ng v·ªÅ nh√≥m)
    s = series.astype(str).str.strip().str.upper()
    s = s.map(lambda x: config.RAW_TO_CANONICAL.get(x, "OTHER"))
    
    # 2. Canonical -> Binary (ho·∫∑c gi·ªØ nguy√™n)
    # L∆∞u √Ω: Logic c≈© c·ªßa b·∫°n d·ª±a v√†o config.BINARY_MODE global. 
    # ·ªû ƒë√¢y ta ∆∞u ti√™n tham s·ªë truy·ªÅn v√†o, n·∫øu kh√¥ng c√≥ th√¨ fallback v·ªÅ config (ho·∫∑c True m·∫∑c ƒë·ªãnh).
    if binary_mode:
        # 0: BENIGN, 1: Attack
        return s.apply(lambda x: 0 if x == "BENIGN" else 1)
    
    return s

def get_scaler():
    return StandardScaler()

# =========================================================================
# H√ÄM 1: Load d·ªØ li·ªáu m·∫´u ƒë·ªÉ ch·∫°y Feature Selection (mRMR)
# =========================================================================
def load_data_for_mrmr(sample_size=200000):
    """
    Load m·∫´u ng·∫´u nhi√™n t·ª´ c·∫£ 2 b·ªô dataset.
    D√πng ƒë·ªÉ ch·∫°y thu·∫≠t to√°n mRMR.
    """
    files = list(config.DIR_2017.glob("*.csv")) + list(config.DIR_2018.glob("*.csv"))
    if not files: raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y file CSV!")
    
    samples_per_file = max(1, sample_size // len(files))
    
    # L·∫•y danh s√°ch c·ªôt chu·∫©n t·ª´ 1 file 2017 (ƒë·ªÉ l√†m m·ªëc validation)
    try:
        sample_2017 = pd.read_csv(next(config.DIR_2017.glob("*.csv")), nrows=1)
        sample_2017 = normalize_column_names(sample_2017)
        valid_cols = [c for c in sample_2017.columns if "Label" not in c and "Class" not in c]
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c m·∫´u 2017: {e}")
        return None, None, None

    X_list = []
    y_list = []
    
    print(f"üöÄ Sampling data ({sample_size} rows) for mRMR...")
    
    for f in files:
        try:
            # ƒê·ªçc chunk l·ªõn h∆°n c·∫ßn thi·∫øt ch√∫t ƒë·ªÉ random
            chunk = pd.read_csv(f, nrows=samples_per_file * 2, low_memory=False)
            
            # 1. Chu·∫©n h√≥a t√™n & X√≥a c·ªôt r√°c
            chunk = normalize_column_names(chunk)
            
            # 2. T√¨m Label
            label_col = next((c for c in ["Label", "Class", "label", "class"] if c in chunk.columns), None)
            if not label_col: continue
            
            # 3. Chu·∫©n h√≥a Label (M·∫∑c ƒë·ªãnh Binary cho mRMR)
            y_chunk = normalize_labels(chunk[label_col], binary_mode=True)
            
            # 4. √âp c·ªôt theo chu·∫©n (ƒêi·ªÅn 0 n·∫øu thi·∫øu)
            for col in valid_cols:
                if col not in chunk.columns:
                    chunk[col] = 0.0
            
            X_chunk = chunk[valid_cols]
            
            # 5. L·∫•y m·∫´u ng·∫´u nhi√™n
            if len(X_chunk) > samples_per_file:
                indices = np.random.choice(len(X_chunk), samples_per_file, replace=False)
                X_chunk = X_chunk.iloc[indices]
                y_chunk = y_chunk.iloc[indices]
            
            # Clean NaN
            X_chunk = X_chunk.replace([np.inf, -np.inf], np.nan).fillna(0)
            
            X_list.append(X_chunk)
            y_list.append(y_chunk)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error reading {f.name}: {e}")

    X_final = pd.concat(X_list, ignore_index=True)
    y_final = pd.concat(y_list, ignore_index=True)
    
    print(f"‚úÖ Data for mRMR Ready: {X_final.shape}")
    return X_final, y_final, valid_cols

# =========================================================================
# H√ÄM 2: Load d·ªØ li·ªáu ƒë·ªÉ Train Model (Chunking Mode)
# =========================================================================
def load_mixed_datasets_chunked(binary_mode=True):
    """
    Load to√†n b·ªô d·ªØ li·ªáu (theo chunk) ƒë·ªÉ train.
    Ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt trong config.SELECTED_FEATURES.
    ƒê·∫£m b·∫£o th·ª© t·ª± c·ªôt c·ªë ƒë·ªãnh cho Autoencoder.
    """
    print(f"üîÑ Loading mixed datasets (Chunking Mode, Binary={binary_mode})...")
    files = list(config.DIR_2017.glob("*.csv")) + list(config.DIR_2018.glob("*.csv"))
    
    X_list = []
    y_list = []
    
    for f in files:
        # print(f"Processing {f.name}...") 
        for chunk in pd.read_csv(f, chunksize=config.CHUNK_SIZE, low_memory=False):
            try:
                # 1. Chu·∫©n h√≥a t√™n & X√≥a c·ªôt r√°c
                chunk = normalize_column_names(chunk)
                
                # 2. T√¨m Label
                label_col = next((c for c in ["Label", "Class", "label", "class"] if c in chunk.columns), None)
                if not label_col: continue
                
                # 3. Chu·∫©n h√≥a Label
                y_chunk = normalize_labels(chunk[label_col], binary_mode=binary_mode)
                
                # 4. L·ªçc Feature & Fill Missing (QUAN TR·ªåNG)
                # Ph·∫£i d√πng ƒë√∫ng danh s√°ch 67 features trong config ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª±
                for col in config.SELECTED_FEATURES:
                    if col not in chunk.columns:
                        chunk[col] = 0.0
                
                # √âp l·∫•y ƒë√∫ng th·ª© t·ª± c·ªôt trong config
                X_chunk = chunk[config.SELECTED_FEATURES]
                
                # X·ª≠ l√Ω v√¥ c√πng v√† NaN
                X_chunk = X_chunk.replace([np.inf, -np.inf], np.nan).fillna(0)
                
                X_list.append(X_chunk)
                y_list.append(y_chunk)
            except Exception as e:
                print(f"‚ö†Ô∏è Error chunk in {f.name}: {e}")
                continue
            
    if not X_list:
        raise ValueError("Kh√¥ng load ƒë∆∞·ª£c d·ªØ li·ªáu n√†o! Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")

    X_final = pd.concat(X_list, ignore_index=True)
    y_final = pd.concat(y_list, ignore_index=True)
    
    print(f"‚úÖ Loaded Mixed Total: {X_final.shape}")
    print(f"   Labels distribution:\n{y_final.value_counts()}")
    
    # Tr·∫£ v·ªÅ numpy array
    return X_final.values, y_final.values

# =========================================================================
# H√ÄM 3 (M·ªöI): Load ri√™ng dataset nƒÉm 2017 ho·∫∑c 2018
# =========================================================================
def load_single_dataset_year(year, binary_mode=True):
    """
    Load ri√™ng dataset nƒÉm 2017 ho·∫∑c 2018.
    √Åp d·ª•ng to√†n b·ªô quy tr√¨nh chu·∫©n h√≥a (rename 2018->2017, filter features)
    ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch v·ªõi model ƒë√£ train tr√™n t·∫≠p mixed.
    
    Args:
        year: '2017' ho·∫∑c '2018' (string ho·∫∑c int)
        binary_mode: True/False
    
    Returns:
        X (DataFrame), y (Series) - Tr·∫£ v·ªÅ DataFrame ƒë·ªÉ d·ªÖ split/analyze
    """
    print(f"üîÑ Loading dataset year {year} (Binary={binary_mode})...")
    
    # 1. X√°c ƒë·ªãnh file
    if str(year) == '2017':
        files = list(config.DIR_2017.glob("*.csv"))
    elif str(year) == '2018':
        files = list(config.DIR_2018.glob("*.csv"))
    else:
        raise ValueError("Year must be '2017' or '2018'")
        
    if not files:
        raise FileNotFoundError(f"No files found for year {year}")

    X_list = []
    y_list = []
    
    for f in files:
        # print(f"  - Reading {f.name}...")
        try:
            # D√πng chunksize ƒë·ªÉ tr√°nh tr√†n RAM n·∫øu file l·ªõn
            for chunk in pd.read_csv(f, chunksize=config.CHUNK_SIZE, low_memory=False):
                
                # --- QUY TR√åNH GI·ªêNG H·ªÜT LOAD_MIXED ---
                
                # 1. Chu·∫©n h√≥a t√™n (bao g·ªìm rename 2018 -> 2017)
                chunk = normalize_column_names(chunk)
                
                # 2. T√¨m Label
                label_col = next((c for c in ["Label", "Class", "label", "class"] if c in chunk.columns), None)
                if not label_col: continue
                
                # 3. Chu·∫©n h√≥a Label
                y_chunk = normalize_labels(chunk[label_col], binary_mode=binary_mode)
                
                # 4. L·ªçc Features (B·∫Øt bu·ªôc ph·∫£i kh·ªõp config.SELECTED_FEATURES)
                for col in config.SELECTED_FEATURES:
                    if col not in chunk.columns:
                        chunk[col] = 0.0
                
                X_chunk = chunk[config.SELECTED_FEATURES]
                X_chunk = X_chunk.replace([np.inf, -np.inf], np.nan).fillna(0)
                
                X_list.append(X_chunk)
                y_list.append(y_chunk)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error reading {f.name}: {e}")

    if not X_list:
        raise ValueError(f"Kh√¥ng load ƒë∆∞·ª£c d·ªØ li·ªáu n√†o cho nƒÉm {year}")

    X_final = pd.concat(X_list, ignore_index=True)
    y_final = pd.concat(y_list, ignore_index=True)
    
    print(f"‚úÖ Loaded {year}. Shape: {X_final.shape}")
    
    # Tr·∫£ v·ªÅ DataFrame/Series ƒë·ªÉ d·ªÖ d√†ng train_test_split v√† debug trong notebook
    return X_final, y_final




# Old version deleted below:

# # src/preprocessing.py
# import pandas as pd
# import numpy as np
# import glob
# from sklearn.preprocessing import StandardScaler
# from . import config

# def normalize_column_names(df):
#     """
#     Chu·∫©n h√≥a t√™n c·ªôt:
#     1. X√≥a kho·∫£ng tr·∫Øng (Strip)
#     2. ƒê·ªïi t√™n c·ªôt 2018 -> 2017 (d·ª±a tr√™n config)
#     3. X√≥a c√°c c·ªôt trong DROP_COLS (Identifier, Sparse, Duplicate)
#     """
#     # 1. Strip whitespace
#     df.columns = df.columns.str.strip()
    
#     # 2. Rename (2018 -> 2017)
#     df = df.rename(columns=config.RENAME_2018_TO_2017)
    
#     # 3. Drop Columns (n·∫øu t·ªìn t·∫°i)
#     cols_to_drop = [c for c in config.DROP_COLS if c in df.columns]
#     if cols_to_drop:
#         df.drop(columns=cols_to_drop, inplace=True)
        
#     return df

# def normalize_labels(series):
#     """Chu·∫©n h√≥a nh√£n t·ª´ Raw -> Canonical -> Binary/Multi"""
#     s = series.astype(str).str.strip().str.upper()
#     s = s.map(lambda x: config.RAW_TO_CANONICAL.get(x, "OTHER"))
    
#     if config.BINARY_MODE:
#         return s.apply(lambda x: 0 if x == "BENIGN" else 1)
#     return s

# def get_scaler():
#     return StandardScaler()

# # =========================================================================
# # H√ÄM 1: Load d·ªØ li·ªáu m·∫´u ƒë·ªÉ ch·∫°y Feature Selection (mRMR)
# # =========================================================================
# def load_data_for_mrmr(sample_size=200000):
#     """
#     Load m·∫´u ng·∫´u nhi√™n t·ª´ c·∫£ 2 b·ªô dataset, t·ª± ƒë·ªông ƒë·ªìng b·ªô t√™n c·ªôt.
#     D√πng ƒë·ªÉ ch·∫°y thu·∫≠t to√°n mRMR.
#     """
#     files = list(config.DIR_2017.glob("*.csv")) + list(config.DIR_2018.glob("*.csv"))
#     if not files: raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y file CSV!")
    
#     samples_per_file = max(1, sample_size // len(files))
    
#     # L·∫•y danh s√°ch c·ªôt chu·∫©n t·ª´ 1 file 2017 (ƒë·ªÉ l√†m m·ªëc)
#     try:
#         sample_2017 = pd.read_csv(next(config.DIR_2017.glob("*.csv")), nrows=1)
#         sample_2017 = normalize_column_names(sample_2017)
#         # C√°c c·ªôt h·ª£p l·ªá l√† c·ªôt c√≤n l·∫°i sau khi drop v√† kh√¥ng ph·∫£i Label
#         valid_cols = [c for c in sample_2017.columns if "Label" not in c and "Class" not in c]
#     except Exception as e:
#         print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c m·∫´u 2017: {e}")
#         return None, None, None

#     X_list = []
#     y_list = []
    
#     print(f"üöÄ Sampling data ({sample_size} rows) for mRMR...")
    
#     for f in files:
#         try:
#             # ƒê·ªçc 1 chunk nh·ªè
#             chunk = pd.read_csv(f, nrows=samples_per_file * 2, low_memory=False)
            
#             # 1. Chu·∫©n h√≥a t√™n & X√≥a c·ªôt r√°c
#             chunk = normalize_column_names(chunk)
            
#             # 2. T√¨m Label
#             label_col = next((c for c in ["Label", "Class", "label", "class"] if c in chunk.columns), None)
#             if not label_col: continue
            
#             # 3. Chu·∫©n h√≥a Label
#             y_chunk = normalize_labels(chunk[label_col])
            
#             # 4. √âp c·ªôt theo chu·∫©n (ƒêi·ªÅn 0 n·∫øu thi·∫øu)
#             for col in valid_cols:
#                 if col not in chunk.columns:
#                     chunk[col] = 0.0
            
#             X_chunk = chunk[valid_cols]
            
#             # 5. L·∫•y m·∫´u ng·∫´u nhi√™n
#             if len(X_chunk) > samples_per_file:
#                 indices = np.random.choice(len(X_chunk), samples_per_file, replace=False)
#                 X_chunk = X_chunk.iloc[indices]
#                 y_chunk = y_chunk.iloc[indices]
            
#             # Clean NaN
#             X_chunk = X_chunk.replace([np.inf, -np.inf], np.nan).fillna(0)
            
#             X_list.append(X_chunk)
#             y_list.append(y_chunk)
            
#         except Exception as e:
#             print(f"‚ö†Ô∏è Error reading {f.name}: {e}")

#     X_final = pd.concat(X_list, ignore_index=True)
#     y_final = pd.concat(y_list, ignore_index=True)
    
#     print(f"‚úÖ Data for mRMR Ready: {X_final.shape}")
#     return X_final, y_final, valid_cols

# # =========================================================================
# # H√ÄM 2: Load d·ªØ li·ªáu ƒë·ªÉ Train Model (Chunking Mode)
# # =========================================================================
# def load_mixed_datasets_chunked():
#     """
#     Load to√†n b·ªô d·ªØ li·ªáu (theo chunk) ƒë·ªÉ train.
#     Ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt trong config.SELECTED_FEATURES.
#     """
#     print(f"üîÑ Loading mixed datasets (Chunking Mode)...")
#     files = list(config.DIR_2017.glob("*.csv")) + list(config.DIR_2018.glob("*.csv"))
    
#     X_list = []
#     y_list = []
    
#     for f in files:
#         # print(f"   üìÑ Reading: {f.name}...")
#         for chunk in pd.read_csv(f, chunksize=config.CHUNK_SIZE, low_memory=False):
#             # 1. Chu·∫©n h√≥a t√™n & X√≥a c·ªôt r√°c
#             chunk = normalize_column_names(chunk)
            
#             # 2. T√¨m Label
#             label_col = next((c for c in ["Label", "Class", "label", "class"] if c in chunk.columns), None)
#             if not label_col: continue
            
#             y_chunk = normalize_labels(chunk[label_col])
            
#             # 3. L·ªçc Feature (Ch·ªâ l·∫•y c·ªôt ƒë√£ ch·ªçn t·ª´ mRMR)
#             # N·∫øu thi·∫øu c·ªôt (do kh√°c version) -> ƒëi·ªÅn 0
#             for col in config.SELECTED_FEATURES:
#                 if col not in chunk.columns:
#                     chunk[col] = 0.0
            
#             X_chunk = chunk[config.SELECTED_FEATURES]
#             X_chunk = X_chunk.replace([np.inf, -np.inf], np.nan).fillna(0)
            
#             X_list.append(X_chunk)
#             y_list.append(y_chunk)
            
#     X_final = pd.concat(X_list, ignore_index=True)
#     y_final = pd.concat(y_list, ignore_index=True)
    
#     print(f"‚úÖ Loaded Total: {X_final.shape}")
#     print(f"   Labels:\n{y_final.value_counts()}")
#     return X_final.values, y_final.values


# # =========================================================================
# # H√ÄM 3: Load ri√™ng dataset nƒÉm 2017 ho·∫∑c 2018
# # =========================================================================
# def load_single_dataset_year(year, binary_mode=True):
#     """
#     Load ri√™ng dataset nƒÉm 2017 ho·∫∑c 2018.
#     year: '2017' ho·∫∑c '2018'
#     """
#     import glob
#     import os
    
#     # 1. X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n file
#     if year == '2017':
#         file_pattern = str(config.DATASET_CIC_IDS2017_DIR / "*.csv")
#     elif year == '2018':
#         file_pattern = str(config.DATASET_CSE_CIC_IDS2018_DIR / "*.csv")
#     else:
#         raise ValueError("Year must be '2017' or '2018'")
        
#     csv_files = glob.glob(file_pattern)
#     print(f"üìÇ T√¨m th·∫•y {len(csv_files)} files cho dataset {year}")

#     X_list = []
#     y_list = []
    
#     # 2. Load t·ª´ng chunk v√† chu·∫©n h√≥a
#     for f in csv_files:
#         print(f"  - Reading {os.path.basename(f)}...")
#         try:
#             for chunk in pd.read_csv(f, chunksize=100000, low_memory=False):
#                 # --- QUAN TR·ªåNG: CHU·∫®N H√ìA T√äN C·ªòT (SCHEMA STANDARDIZATION) ---
#                 chunk.columns = chunk.columns.str.strip() # X√≥a kho·∫£ng tr·∫Øng
                
#                 # Fix l·ªói t√™n c·ªôt c·ª• th·ªÉ c·ªßa 2018 vs 2017 n·∫øu c√≥ (v√≠ d·ª• Timestamp)
#                 # Nh∆∞ng quan tr·ªçng nh·∫•t l√† strip()
                
#                 # T√¨m c·ªôt label
#                 label_col = next((c for c in ["Label", "Class", "label", "class"] if c in chunk.columns), None)
#                 if not label_col: continue
                
#                 # X·ª≠ l√Ω nh√£n
#                 y_chunk, _ = normalize_labels(chunk[label_col], binary_mode=binary_mode)
                
#                 # X·ª≠ l√Ω features (B·ªè c·ªôt r√°c + c·ªôt ƒë·ªãnh danh)
#                 X_chunk = process_and_clean_data(chunk.drop(columns=[label_col]))
                
#                 # --- L·ªåC B·ªöT ƒê·ªÇ TR√ÅNH TR√ÄN RAM (Optional) ---
#                 # N·∫øu ch·ªâ test code, c√≥ th·ªÉ l·∫•y sample. N·∫øu ch·∫°y th·∫≠t, comment d√≤ng d∆∞·ªõi.
#                 # chunk = chunk.sample(frac=0.1) 

#                 X_list.append(X_chunk)
#                 y_list.append(y_chunk)
                
#         except Exception as e:
#             print(f"‚ö†Ô∏è L·ªói ƒë·ªçc file {f}: {e}")

#     # 3. G·ªôp l·∫°i
#     if not X_list:
#         raise ValueError(f"Kh√¥ng load ƒë∆∞·ª£c d·ªØ li·ªáu n√†o cho nƒÉm {year}")

#     X = pd.concat(X_list, ignore_index=True)
#     y = pd.concat(y_list, ignore_index=True)
    
#     # ƒê·∫£m b·∫£o fillna l·∫ßn cu·ªëi
#     X = X.fillna(0)
    
#     print(f"‚úÖ Load xong {year}. Shape: {X.shape}")
#     return X, y